------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computer Vision and Pattern Recognition
 received from  Tue  9 Apr 24 18:00:00 GMT  to  Wed 10 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.06542
Date: Tue, 9 Apr 2024 18:00:25 GMT   (4354kb,D)

Title: Training-Free Open-Vocabulary Segmentation with Offline
  Diffusion-Augmented Prototype Generation
Authors: Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi,
  Rita Cucchiara
Categories: cs.CV
Comments: CVPR 2024. Project page: https://aimagelab.github.io/freeda/
\\
  Open-vocabulary semantic segmentation aims at segmenting arbitrary categories
expressed in textual form. Previous works have trained over large amounts of
image-caption pairs to enforce pixel-level multimodal alignments. However,
captions provide global information about the semantics of a given image but
lack direct localization of individual concepts. Further, training on
large-scale datasets inevitably brings significant computational costs. In this
paper, we propose FreeDA, a training-free diffusion-augmented method for
open-vocabulary semantic segmentation, which leverages the ability of diffusion
models to visually localize generated concepts and local-global similarities to
match class-agnostic regions with semantic classes. Our approach involves an
offline stage in which textual-visual reference embeddings are collected,
starting from a large set of captions and leveraging visual and semantic
contexts. At test time, these are queried to support the visual matching
process, which is carried out by jointly considering class-agnostic regions and
global semantic similarities. Extensive analyses demonstrate that FreeDA
achieves state-of-the-art performance on five datasets, surpassing previous
methods by more than 7.0 average points in terms of mIoU and without requiring
any training.
\\ ( https://arxiv.org/abs/2404.06542 ,  4354kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06559
Date: Tue, 9 Apr 2024 18:23:34 GMT   (2148kb,D)

Title: The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios
Authors: Richard E. Neddo, Zander W. Blasingame, Chen Liu
Categories: cs.CV
Comments: Initial preprint. Under review
\\
  Face morphing attacks present an emerging threat to the face recognition
system. On top of that, printing and scanning the morphed images could obscure
the artifacts generated during the morphing process, which makes morphed image
detection even harder. In this work, we investigate the impact that printing
and scanning has on morphing attacks through a series of heterogeneous tests.
Our experiments show that we can increase the possibility of a false match by
up to 5.64% for DiM and 16.00% for StyleGAN2 when providing an image that has
been printed and scanned, regardless it is morphed or bona fide, to a Face
Recognition (FR) system. Likewise, using Frechet Inception Distance (FID)
metric, strictly print-scanned morph attacks performed on average 9.185%
stronger than non-print-scanned digital morphs.
\\ ( https://arxiv.org/abs/2404.06559 ,  2148kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06564
Date: Tue, 9 Apr 2024 18:28:55 GMT   (2741kb,D)

Title: MambaAD: Exploring State Space Models for Multi-class Unsupervised
  Anomaly Detection
Authors: Haoyang He, Yuhu Bai, Jiangning Zhang, Qingdong He, Hongxu Chen,
  Zhenye Gan, Chengjie Wang, Xiangtai Li, Guanzhong Tian, Lei Xie
Categories: cs.CV
\\
  Recent advancements in anomaly detection have seen the efficacy of CNN- and
transformer-based approaches. However, CNNs struggle with long-range
dependencies, while transformers are burdened by quadratic computational
complexity. Mamba-based models, with their superior long-range modeling and
linear efficiency, have garnered substantial attention. This study pioneers the
application of Mamba to multi-class unsupervised anomaly detection, presenting
MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring
Locality-Enhanced State Space (LSS) modules at multi-scales. The proposed LSS
module, integrating parallel cascaded (Hybrid State Space) HSS blocks and
multi-kernel convolutions operations, effectively captures both long-range and
local information. The HSS block, utilizing (Hybrid Scanning) HS encoders,
encodes feature maps into five scanning methods and eight directions, thereby
strengthening global connections through the (State Space Model) SSM. The use
of Hilbert scanning and eight directions significantly improves feature
sequence modeling. Comprehensive experiments on six diverse anomaly detection
datasets and seven metrics demonstrate SoTA performance, substantiating the
method's effectiveness.
\\ ( https://arxiv.org/abs/2404.06564 ,  2741kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06593
Date: Tue, 9 Apr 2024 19:49:01 GMT   (4864kb,D)

Title: Spatially Optimized Compact Deep Metric Learning Model for Similarity
  Search
Authors: Md. Farhadul Islam, Md. Tanzim Reza, Meem Arafat Manab, Mohammad
  Rakibul Hasan Mahin, Sarah Zabeen, Jannatun Noor
Categories: cs.CV cs.AI cs.LG
Comments: 5 pages, 3 figures,
MSC-class: 68
ACM-class: I.4.7; I.2.6; I.2.10
\\
  Spatial optimization is often overlooked in many computer vision tasks.
Filters should be able to recognize the features of an object regardless of
where it is in the image. Similarity search is a crucial task where spatial
features decide an important output. The capacity of convolution to capture
visual patterns across various locations is limited. In contrast to
convolution, the involution kernel is dynamically created at each pixel based
on the pixel value and parameters that have been learned. This study
demonstrates that utilizing a single layer of involution feature extractor
alongside a compact convolution model significantly enhances the performance of
similarity search. Additionally, we improve predictions by using the GELU
activation function rather than the ReLU. The negligible amount of weight
parameters in involution with a compact model with better performance makes the
model very useful in real-world implementations. Our proposed model is below 1
megabyte in size. We have experimented with our proposed methodology and other
models on CIFAR-10, FashionMNIST, and MNIST datasets. Our proposed method
outperforms across all three datasets.
\\ ( https://arxiv.org/abs/2404.06593 ,  4864kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06605
Date: Tue, 9 Apr 2024 20:24:29 GMT   (9451kb,D)

Title: RoadBEV: Road Surface Reconstruction in Bird's Eye View
Authors: Tong Zhao, Lei Yang, Yichen Xie, Mingyu Ding, Masayoshi Tomizuka,
  Yintao Wei
Categories: cs.CV
Comments: Dataset page: https://thu-rsxd.com/rsrd Code:
  https://github.com/ztsrxh/RoadBEV
\\
  Road surface conditions, especially geometry profiles, enormously affect
driving performance of autonomous vehicles. Vision-based online road
reconstruction promisingly captures road information in advance. Existing
solutions like monocular depth estimation and stereo matching suffer from
modest performance. The recent technique of Bird's-Eye-View (BEV) perception
provides immense potential to more reliable and accurate reconstruction. This
paper uniformly proposes two simple yet effective models for road elevation
reconstruction in BEV named RoadBEV-mono and RoadBEV-stereo, which estimate
road elevation with monocular and stereo images, respectively. The former
directly fits elevation values based on voxel features queried from image view,
while the latter efficiently recognizes road elevation patterns based on BEV
volume representing discrepancy between left and right voxel features.
Insightful analyses reveal their consistence and difference with perspective
view. Experiments on real-world dataset verify the models' effectiveness and
superiority. Elevation errors of RoadBEV-mono and RoadBEV-stereo achieve 1.83cm
and 0.56cm, respectively. The estimation performance improves by 50\% in BEV
based on monocular image. Our models are promising for practical applications,
providing valuable references for vision-based BEV perception in autonomous
driving. The code is released at https://github.com/ztsrxh/RoadBEV.
\\ ( https://arxiv.org/abs/2404.06605 ,  9451kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06622
Date: Tue, 9 Apr 2024 21:12:31 GMT   (79kb,D)

Title: Calibrating Higher-Order Statistics for Few-Shot Class-Incremental
  Learning with Pre-trained Vision Transformers
Authors: Dipam Goswami, Bart{\l}omiej Twardowski, Joost van de Weijer
Categories: cs.CV
Comments: Accepted at CLVision workshop (CVPR 2024)
\\
  Few-shot class-incremental learning (FSCIL) aims to adapt the model to new
classes from very few data (5 samples) without forgetting the previously
learned classes. Recent works in many-shot CIL (MSCIL) (using all available
training data) exploited pre-trained models to reduce forgetting and achieve
better plasticity. In a similar fashion, we use ViT models pre-trained on
large-scale datasets for few-shot settings, which face the critical issue of
low plasticity. FSCIL methods start with a many-shot first task to learn a very
good feature extractor and then move to the few-shot setting from the second
task onwards. While the focus of most recent studies is on how to learn the
many-shot first task so that the model generalizes to all future few-shot
tasks, we explore in this work how to better model the few-shot data using
pre-trained models, irrespective of how the first task is trained. Inspired by
recent works in MSCIL, we explore how using higher-order feature statistics can
influence the classification of few-shot classes. We identify the main
challenge of obtaining a good covariance matrix from few-shot data and propose
to calibrate the covariance matrix for new classes based on semantic similarity
to the many-shot base classes. Using the calibrated feature statistics in
combination with existing methods significantly improves few-shot continual
classification on several FSCIL benchmarks. Code is available at
https://github.com/dipamgoswami/FSCIL-Calibration.
\\ ( https://arxiv.org/abs/2404.06622 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06637
Date: Tue, 9 Apr 2024 22:16:34 GMT   (22210kb,D)

Title: GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis
Authors: Srikumar Sastry, Subash Khanal, Aayush Dhakal, Nathan Jacobs
Categories: cs.CV
\\
  We present GeoSynth, a model for synthesizing satellite images with global
style and image-driven layout control. The global style control is via textual
prompts or geographic location. These enable the specification of scene
semantics or regional appearance respectively, and can be used together. We
train our model on a large dataset of paired satellite imagery, with
automatically generated captions, and OpenStreetMap data. We evaluate various
combinations of control inputs, including different types of layout controls.
Results demonstrate that our model can generate diverse, high-quality images
and exhibits excellent zero-shot generalization. The code and model checkpoints
are available at https://github.com/mvrl/GeoSynth.
\\ ( https://arxiv.org/abs/2404.06637 ,  22210kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06653
Date: Tue, 9 Apr 2024 23:24:19 GMT   (5327kb,D)

Title: FlameFinder: Illuminating Obscured Fire through Smoke with Attentive
  Deep Metric Learning
Authors: Hossein Rajoli, Sahand Khoshdel, Fatemeh Afghah, Xiaolong Ma
Categories: cs.CV
Comments: Submitted as a Journal Paper to IEEE Transactions on Geoscience and
  Remote Sensing
\\
  FlameFinder is a deep metric learning (DML) framework designed to accurately
detect flames, even when obscured by smoke, using thermal images from
firefighter drones during wildfire monitoring. Traditional RGB cameras struggle
in such conditions, but thermal cameras can capture smoke-obscured flame
features. However, they lack absolute thermal reference points, leading to
false positives.To address this issue, FlameFinder utilizes paired thermal-RGB
images for training. By learning latent flame features from smoke-free samples,
the model becomes less biased towards relative thermal gradients. In testing,
it identifies flames in smoky patches by analyzing their equivalent
thermal-domain distribution. This method improves performance using both
supervised and distance-based clustering metrics.The framework incorporates a
flame segmentation method and a DML-aided detection framework. This includes
utilizing center loss (CL), triplet center loss (TCL), and triplet cosine
center loss (TCCL) to identify optimal cluster representatives for
classification. However, the dominance of center loss over the other losses
leads to the model missing features sensitive to them. To address this
limitation, an attention mechanism is proposed. This mechanism allows for
non-uniform feature contribution, amplifying the critical role of cosine and
triplet loss in the DML framework. Additionally, it improves interpretability,
class discrimination, and decreases intra-class variance. As a result, the
proposed model surpasses the baseline by 4.4% in the FLAME2 dataset and 7% in
the FLAME3 dataset for unobscured flame detection accuracy. Moreover, it
demonstrates enhanced class separation in obscured scenarios compared to VGG19,
ResNet18, and three backbone models tailored for flame detection.
\\ ( https://arxiv.org/abs/2404.06653 ,  5327kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06661
Date: Wed, 10 Apr 2024 00:05:55 GMT   (7609kb,D)

Title: Efficient Denoising using Score Embedding in Score-based Diffusion
  Models
Authors: Andrew S. Na and William Gao and Justin W.L. Wan
Categories: cs.CV
\\
  It is well known that training a denoising score-based diffusion models
requires tens of thousands of epochs and a substantial number of image data to
train the model. In this paper, we propose to increase the efficiency in
training score-based diffusion models. Our method allows us to decrease the
number of epochs needed to train the diffusion model. We accomplish this by
solving the log-density Fokker-Planck (FP) Equation numerically to compute the
score \textit{before} training. The pre-computed score is embedded into the
image to encourage faster training under slice Wasserstein distance.
Consequently, it also allows us to decrease the number of images we need to
train the neural network to learn an accurate score. We demonstrate through our
numerical experiments the improved performance of our proposed method compared
to standard score-based diffusion models. Our proposed method achieves a
similar quality to the standard method meaningfully faster.
\\ ( https://arxiv.org/abs/2404.06661 ,  7609kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06663
Date: Wed, 10 Apr 2024 00:11:03 GMT   (4249kb,D)

Title: Multi-modal Document Presentation Attack Detection With Forensics Trace
  Disentanglement
Authors: Changsheng Chen, Yongyi Deng, Liangwei Lin, Zitong Yu, Zhimao Lai
Categories: cs.CV
Comments: Accepted to ICME 2024
\\
  Document Presentation Attack Detection (DPAD) is an important measure in
protecting the authenticity of a document image. However, recent DPAD methods
demand additional resources, such as manual effort in collecting additional
data or knowing the parameters of acquisition devices. This work proposes a
DPAD method based on multi-modal disentangled traces (MMDT) without the above
drawbacks. We first disentangle the recaptured traces by a self-supervised
disentanglement and synthesis network to enhance the generalization capacity in
document images with different contents and layouts. Then, unlike the existing
DPAD approaches that rely only on data in the RGB domain, we propose to
explicitly employ the disentangled recaptured traces as new modalities in the
transformer backbone through adaptive multi-modal adapters to fuse RGB/trace
features efficiently. Visualization of the disentangled traces confirms the
effectiveness of the proposed method in different document contents. Extensive
experiments on three benchmark datasets demonstrate the superiority of our MMDT
method on representing forensic traces of recapturing distortion.
\\ ( https://arxiv.org/abs/2404.06663 ,  4249kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06665
Date: Wed, 10 Apr 2024 00:25:09 GMT   (4543kb,D)

Title: Deep Generative Data Assimilation in Multimodal Setting
Authors: Yongquan Qu, Juan Nathaniel, Shuolin Li, Pierre Gentine
Categories: cs.CV
Comments: Accepted to CVPR2024 EarthVision
\\
  Robust integration of physical knowledge and data is key to improve
computational simulations, such as Earth system models. Data assimilation is
crucial for achieving this goal because it provides a systematic framework to
calibrate model outputs with observations, which can include remote sensing
imagery and ground station measurements, with uncertainty quantification.
Conventional methods, including Kalman filters and variational approaches,
inherently rely on simplifying linear and Gaussian assumptions, and can be
computationally expensive. Nevertheless, with the rapid adoption of data-driven
methods in many areas of computational sciences, we see the potential of
emulating traditional data assimilation with deep learning, especially
generative models. In particular, the diffusion-based probabilistic framework
has large overlaps with data assimilation principles: both allows for
conditional generation of samples with a Bayesian inverse framework. These
models have shown remarkable success in text-conditioned image generation or
image-controlled video synthesis. Likewise, one can frame data assimilation as
observation-conditioned state calibration. In this work, we propose SLAMS:
Score-based Latent Assimilation in Multimodal Setting. Specifically, we
assimilate in-situ weather station data and ex-situ satellite imagery to
calibrate the vertical temperature profiles, globally. Through extensive
ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy,
and sparse data settings. To our knowledge, our work is the first to apply deep
generative framework for multimodal data assimilation using real-world
datasets; an important step for building robust computational simulators,
including the next-generation Earth system models. Our code is available at:
https://github.com/yongquan-qu/SLAMS
\\ ( https://arxiv.org/abs/2404.06665 ,  4543kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06666
Date: Wed, 10 Apr 2024 00:26:08 GMT   (26176kb,D)

Title: SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models
Authors: Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu
  Ji, Wenyuan Xu
Categories: cs.CV cs.AI cs.CL cs.CR
\\
  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited
remarkable performance in generating high-quality images from text descriptions
in recent years. However, text-to-image models may be tricked into generating
not-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing
countermeasures mostly focus on filtering inappropriate inputs and outputs, or
suppressing improper text embeddings, which can block explicit NSFW-related
content (e.g., naked or sexy) but may still be vulnerable to adversarial
prompts inputs that appear innocent but are ill-intended. In this paper, we
present SafeGen, a framework to mitigate unsafe content generation by
text-to-image models in a text-agnostic manner. The key idea is to eliminate
unsafe visual representations from the model regardless of the text input. In
this way, the text-to-image model is resistant to adversarial prompts since
unsafe visual representations are obstructed from within. Extensive experiments
conducted on four datasets demonstrate SafeGen's effectiveness in mitigating
unsafe content generation while preserving the high-fidelity of benign images.
SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1%
sexual content removal performance. Furthermore, our constructed benchmark of
adversarial prompts provides a basis for future development and evaluation of
anti-NSFW-generation methods.
\\ ( https://arxiv.org/abs/2404.06666 ,  26176kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06683
Date: Wed, 10 Apr 2024 02:03:14 GMT   (4038kb,D)

Title: Unsupervised Visible-Infrared ReID via Pseudo-label Correction and
  Modality-level Alignment
Authors: Yexin Liu, Weiming Zhang, Athanasios V. Vasilakos, Lin Wang
Categories: cs.CV
Comments: 10 pages, 6 figures
\\
  Unsupervised visible-infrared person re-identification (UVI-ReID) has
recently gained great attention due to its potential for enhancing human
detection in diverse environments without labeling. Previous methods utilize
intra-modality clustering and cross-modality feature matching to achieve
UVI-ReID. However, there exist two challenges: 1) noisy pseudo labels might be
generated in the clustering process, and 2) the cross-modality feature
alignment via matching the marginal distribution of visible and infrared
modalities may misalign the different identities from two modalities. In this
paper, we first conduct a theoretic analysis where an interpretable
generalization upper bound is introduced. Based on the analysis, we then
propose a novel unsupervised cross-modality person re-identification framework
(PRAISE). Specifically, to address the first challenge, we propose a
pseudo-label correction strategy that utilizes a Beta Mixture Model to predict
the probability of mis-clustering based network's memory effect and rectifies
the correspondence by adding a perceptual term to contrastive learning. Next,
we introduce a modality-level alignment strategy that generates paired
visible-infrared latent features and reduces the modality gap by aligning the
labeling function of visible and infrared features to learn identity
discriminative and modality-invariant features. Experimental results on two
benchmark datasets demonstrate that our method achieves state-of-the-art
performance than the unsupervised visible-ReID methods.
\\ ( https://arxiv.org/abs/2404.06683 ,  4038kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06692
Date: Wed, 10 Apr 2024 02:40:17 GMT   (3660kb,D)

Title: Perception-Oriented Video Frame Interpolation via Asymmetric Blending
Authors: Guangyang Wu, Xin Tao, Changlin Li, Wenyi Wang, Xiaohong Liu, Qingqing
  Zheng
Categories: cs.CV
Comments: Accepted by CVPR 2024
\\
  Previous methods for Video Frame Interpolation (VFI) have encountered
challenges, notably the manifestation of blur and ghosting effects. These
issues can be traced back to two pivotal factors: unavoidable motion errors and
misalignment in supervision. In practice, motion estimates often prove to be
error-prone, resulting in misaligned features. Furthermore, the reconstruction
loss tends to bring blurry results, particularly in misaligned regions. To
mitigate these challenges, we propose a new paradigm called PerVFI
(Perception-oriented Video Frame Interpolation). Our approach incorporates an
Asymmetric Synergistic Blending module (ASB) that utilizes features from both
sides to synergistically blend intermediate features. One reference frame
emphasizes primary content, while the other contributes complementary
information. To impose a stringent constraint on the blending process, we
introduce a self-learned sparse quasi-binary mask which effectively mitigates
ghosting and blur artifacts in the output. Additionally, we employ a
normalizing flow-based generator and utilize the negative log-likelihood loss
to learn the conditional distribution of the output, which further facilitates
the generation of clear and fine details. Experimental results validate the
superiority of PerVFI, demonstrating significant improvements in perceptual
quality compared to existing methods. Codes are available at
\url{https://github.com/mulns/PerVFI}
\\ ( https://arxiv.org/abs/2404.06692 ,  3660kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06693
Date: Wed, 10 Apr 2024 02:47:05 GMT   (4528kb,D)

Title: Binomial Self-compensation for Motion Error in Dynamic 3D Scanning
Authors: Geyou Zhang, Ce Zhu, and Kai Liu
Categories: cs.CV eess.IV
\\
  Phase shifting profilometry (PSP) is favored in high-precision 3D scanning
due to its high accuracy, robustness, and pixel-wise property. However, a
fundamental assumption of PSP that the object should remain static is violated
in dynamic measurement, making PSP susceptible to object moving, resulting in
ripple-like errors in the point clouds. We propose a pixel-wise and frame-wise
loopable binomial self-compensation (BSC) algorithm to effectively and flexibly
eliminate motion error in the four-step PSP. Our mathematical model
demonstrates that by summing successive motion-affected phase frames weighted
by binomial coefficients, motion error exponentially diminishes as the binomial
order increases, accomplishing automatic error compensation through the
motion-affected phase sequence, without the assistance of any intermediate
variable. Extensive experiments show that our BSC outperforms the existing
methods in reducing motion error, while achieving a depth map frame rate equal
to the camera's acquisition rate (90 fps), enabling high-accuracy 3D
reconstruction with a quasi-single-shot frame rate.
\\ ( https://arxiv.org/abs/2404.06693 ,  4528kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06700
Date: Wed, 10 Apr 2024 03:11:10 GMT   (3236kb,D)

Title: Scaling Multi-Camera 3D Object Detection through Weak-to-Strong
  Eliciting
Authors: Hao Lu, Jiaqi Tang, Xinli Xu, Xu Cao, Yunpeng Zhang, Guoqing Wang,
  Dalong Du, Hao Chen, Yingcong Chen
Categories: cs.CV
\\
  The emergence of Multi-Camera 3D Object Detection (MC3D-Det), facilitated by
bird's-eye view (BEV) representation, signifies a notable progression in 3D
object detection. Scaling MC3D-Det training effectively accommodates varied
camera parameters and urban landscapes, paving the way for the MC3D-Det
foundation model. However, the multi-view fusion stage of the MC3D-Det method
relies on the ill-posed monocular perception during training rather than
surround refinement ability, leading to what we term "surround refinement
degradation". To this end, our study presents a weak-to-strong eliciting
framework aimed at enhancing surround refinement while maintaining robust
monocular perception. Specifically, our framework employs weakly tuned experts
trained on distinct subsets, and each is inherently biased toward specific
camera configurations and scenarios. These biased experts can learn the
perception of monocular degeneration, which can help the multi-view fusion
stage to enhance surround refinement abilities. Moreover, a composite
distillation strategy is proposed to integrate the universal knowledge of 2D
foundation models and task-specific information. Finally, for MC3D-Det joint
training, the elaborate dataset merge strategy is designed to solve the problem
of inconsistent camera numbers and camera parameters. We set up a multiple
dataset joint training benchmark for MC3D-Det and adequately evaluated existing
methods. Further, we demonstrate the proposed framework brings a generalized
and significant boost over multiple baselines. Our code is at
\url{https://github.com/EnVision-Research/Scale-BEV}.
\\ ( https://arxiv.org/abs/2404.06700 ,  3236kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06704
Date: Wed, 10 Apr 2024 03:20:33 GMT   (15891kb,D)

Title: Convolution-based Probability Gradient Loss for Semantic Segmentation
Authors: Guohang Shan and Shuangcheng Jia
Categories: cs.CV cs.AI
Comments: 12 pages, 7 figures
\\
  In this paper, we introduce a novel Convolution-based Probability Gradient
(CPG) loss for semantic segmentation. It employs convolution kernels similar to
the Sobel operator, capable of computing the gradient of pixel intensity in an
image. This enables the computation of gradients for both ground-truth and
predicted category-wise probabilities. It enhances network performance by
maximizing the similarity between these two probability gradients. Moreover, to
specifically enhance accuracy near the object's boundary, we extract the object
boundary based on the ground-truth probability gradient and exclusively apply
the CPG loss to pixels belonging to boundaries. CPG loss proves to be highly
convenient and effective. It establishes pixel relationships through
convolution, calculating errors from a distinct dimension compared to
pixel-wise loss functions such as cross-entropy loss. We conduct qualitative
and quantitative analyses to evaluate the impact of the CPG loss on three
well-established networks (DeepLabv3-Resnet50, HRNetV2-OCR, and
LRASPP_MobileNet_V3_Large) across three standard segmentation datasets
(Cityscapes, COCO-Stuff, ADE20K). Our extensive experimental results
consistently and significantly demonstrate that the CPG loss enhances the mean
Intersection over Union.
\\ ( https://arxiv.org/abs/2404.06704 ,  15891kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06710
Date: Wed, 10 Apr 2024 03:31:32 GMT   (23331kb,D)

Title: SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike
  Camera
Authors: Gaole Dai and Zhenyu Wang and Qinwen Xu and Wen Cheng and Ming Lu and
  Boxing Shi and Shanghang Zhang and Tiejun Huang
Categories: cs.CV cs.AI
\\
  One of the most critical factors in achieving sharp Novel View Synthesis
(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) is the quality of the training images. However,
Conventional RGB cameras are susceptible to motion blur. In contrast,
neuromorphic cameras like event and spike cameras inherently capture more
comprehensive temporal information, which can provide a sharp representation of
the scene as additional training data. Recent methods have explored the
integration of event cameras to improve the quality of NVS. The event-RGB
approaches have some limitations, such as high training costs and the inability
to work effectively in the background. Instead, our study introduces a new
method that uses the spike camera to overcome these limitations. By considering
texture reconstruction from spike streams as ground truth, we design the
Texture from Spike (TfS) loss. Since the spike camera relies on temporal
integration instead of temporal differentiation used by event cameras, our
proposed TfS loss maintains manageable training costs. It handles foreground
objects with backgrounds simultaneously. We also provide a real-world dataset
captured with our spike-RGB camera system to facilitate future research
endeavors. We conduct extensive experiments using synthetic and real-world
datasets to demonstrate that our design can enhance novel view synthesis across
NeRF and 3DGS. The code and dataset will be made available for public access.
\\ ( https://arxiv.org/abs/2404.06710 ,  23331kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06715
Date: Wed, 10 Apr 2024 03:54:53 GMT   (40291kb,D)

Title: Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR
  Data
Authors: Aakash Kumar, Chen Chen, Ajmal Mian, Neils Lobo, Mubarak Shah
Categories: cs.CV
\\
  3D detection is a critical task that enables machines to identify and locate
objects in three-dimensional space. It has a broad range of applications in
several fields, including autonomous driving, robotics and augmented reality.
Monocular 3D detection is attractive as it requires only a single camera,
however, it lacks the accuracy and robustness required for real world
applications. High resolution LiDAR on the other hand, can be expensive and
lead to interference problems in heavy traffic given their active
transmissions. We propose a balanced approach that combines the advantages of
monocular and point cloud-based 3D detection. Our method requires only a small
number of 3D points, that can be obtained from a low-cost, low-resolution
sensor. Specifically, we use only 512 points, which is just 1% of a full LiDAR
frame in the KITTI dataset. Our method reconstructs a complete 3D point cloud
from this limited 3D information combined with a single image. The
reconstructed 3D point cloud and corresponding image can be used by any
multi-modal off-the-shelf detector for 3D object detection. By using the
proposed network architecture with an off-the-shelf multi-modal 3D detector,
the accuracy of 3D detection improves by 20% compared to the state-of-the-art
monocular detection methods and 6% to 9% compare to the baseline multi-modal
methods on KITTI and JackRabbot datasets.
\\ ( https://arxiv.org/abs/2404.06715 ,  40291kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06727
Date: Wed, 10 Apr 2024 04:24:42 GMT   (11777kb,D)

Title: Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural
  Radiance Fields
Authors: Sibeak Lee and Kyeongsu Kang and Hyeonwoo Yu
Categories: cs.CV
\\
  We present the Bayesian Neural Radiance Field (NeRF), which explicitly
quantifies uncertainty in geometric volume structures without the need for
additional networks, making it adept for challenging observations and
uncontrolled images. NeRF diverges from traditional geometric methods by
offering an enriched scene representation, rendering color and density in 3D
space from various viewpoints. However, NeRF encounters limitations in relaxing
uncertainties by using geometric structure information, leading to inaccuracies
in interpretation under insufficient real-world observations. Recent research
efforts aimed at addressing this issue have primarily relied on empirical
methods or auxiliary networks. To fundamentally address this issue, we propose
a series of formulational extensions to NeRF. By introducing generalized
approximations and defining density-related uncertainty, our method seamlessly
extends to manage uncertainty not only for RGB but also for depth, without the
need for additional networks or empirical assumptions. In experiments we show
that our method significantly enhances performance on RGB and depth images in
the comprehensive dataset, demonstrating the reliability of the Bayesian NeRF
approach to quantifying uncertainty based on the geometric structure.
\\ ( https://arxiv.org/abs/2404.06727 ,  11777kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06741
Date: Wed, 10 Apr 2024 04:59:51 GMT   (6067kb,D)

Title: An Animation-based Augmentation Approach for Action Recognition from
  Discontinuous Video
Authors: Xingyu Song, Zhan Li, Shi Chen, Xin-Qiang Cai, Kazuyuki Demachi
Categories: cs.CV
\\
  The study of action recognition has attracted considerable attention recently
due to its broad applications in multiple areas. However, with the issue of
discontinuous training video, which not only decreases the performance of
action recognition model, but complicates the data augmentation process as
well, still remains under-exploration. In this study, we introduce the 4A
(Action Animation-based Augmentation Approach), an innovative pipeline for data
augmentation to address the problem. The main contributions remain in our work
includes: (1) we investigate the problem of severe decrease on performance of
action recognition task training by discontinuous video, and the limitation of
existing augmentation methods on solving this problem. (2) we propose a novel
augmentation pipeline, 4A, to address the problem of discontinuous video for
training, while achieving a smoother and natural-looking action representation
than the latest data augmentation methodology. (3) We achieve the same
performance with only 10% of the original data for training as with all of the
original data from the real-world dataset, and a better performance on
In-the-wild videos, by employing our data augmentation techniques.
\\ ( https://arxiv.org/abs/2404.06741 ,  6067kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06744
Date: Wed, 10 Apr 2024 05:10:05 GMT   (19642kb,D)

Title: YOLO based Ocean Eddy Localization with AWS SageMaker
Authors: Seraj Al Mahmud Mostafa, Jinbo Wang, Benjamin Holt, Jianwu Wang
Categories: cs.CV
Comments: 10 pages
\\
  Ocean eddies play a significant role both on the sea surface and beneath it,
contributing to the sustainability of marine life dependent on oceanic
behaviors. Therefore, it is crucial to investigate ocean eddies to monitor
changes in the Earth, particularly in the oceans, and their impact on climate.
This study aims to pinpoint ocean eddies using AWS cloud services, specifically
SageMaker. The primary objective is to detect small-scale (<20km) ocean eddies
from satellite remote images and assess the feasibility of utilizing SageMaker,
which offers tools for deploying AI applications. Moreover, this research not
only explores the deployment of cloud-based services for remote sensing of
Earth data but also evaluates several YOLO (You Only Look Once) models using
single and multi-GPU-based services in the cloud. Furthermore, this study
underscores the potential of these services, their limitations, challenges
related to deployment and resource management, and their user-riendliness for
Earth science projects.
\\ ( https://arxiv.org/abs/2404.06744 ,  19642kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06753
Date: Wed, 10 Apr 2024 05:41:05 GMT   (45466kb,D)

Title: MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D
  Reconstruction of Indoor Scenes from Monocular RGB Views
Authors: Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen
Categories: cs.CV
\\
  Current monocular 3D scene reconstruction (3DR) works are either
fully-supervised, or not generalizable, or implicit in 3D representation. We
propose a novel framework - MonoSelfRecon that for the first time achieves
explicit 3D mesh reconstruction for generalizable indoor scenes with monocular
RGB views by purely self-supervision on voxel-SDF (signed distance function).
MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and
a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF
in self-supervision. We propose novel self-supervised losses, which not only
support pure self-supervision, but can be used together with supervised signals
to further boost supervised training. Our experiments show that "MonoSelfRecon"
trained in pure self-supervision outperforms current best self-supervised
indoor depth estimation models and is comparable to 3DR models trained in fully
supervision with depth annotations. MonoSelfRecon is not restricted by specific
model design, which can be used to any models with voxel-SDF for purely
self-supervised manner.
\\ ( https://arxiv.org/abs/2404.06753 ,  45466kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06773
Date: Wed, 10 Apr 2024 06:30:08 GMT   (1729kb,D)

Title: Adapting LLaMA Decoder to Vision Transformer
Authors: Jiahao Wang and Wenqi Shao and Mengzhao Chen and Chengyue Wu and Yong
  Liu and Kaipeng Zhang and Songyang Zhang and Kai Chen and Ping Luo
Categories: cs.CV
Comments: 22 pages, 10 figures
\\
  This work examines whether decoder-only Transformers such as LLaMA, which
were originally designed for large language models (LLMs), can be adapted to
the computer vision field. We first "LLaMAfy" a standard ViT step-by-step to
align with LLaMA's architecture, and find that directly applying a casual mask
to the self-attention brings an attention collapse issue, resulting in the
failure to the network training. We suggest to reposition the class token
behind the image tokens with a post-sequence class token technique to overcome
this challenge, enabling causal self-attention to efficiently capture the
entire image's information. Additionally, we develop a soft mask strategy that
gradually introduces a casual mask to the self-attention at the onset of
training to facilitate the optimization behavior. The tailored model, dubbed as
image LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct
supervised learning. Its causal self-attention boosts computational efficiency
and learns complex representation by elevating attention map ranks. iLLaMA
rivals the performance with its encoder-only counterparts, achieving 75.1%
ImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to ~310M
and pre-training on ImageNet-21K further enhances the accuracy to 86.0%.
Extensive experiments demonstrate iLLaMA's reliable properties: calibration,
shape-texture bias, quantization compatibility, ADE20K segmentation and CIFAR
transfer learning. We hope our study can kindle fresh views to visual model
design in the wave of LLMs. Pre-trained models and codes are available here.
\\ ( https://arxiv.org/abs/2404.06773 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06779
Date: Wed, 10 Apr 2024 06:39:18 GMT   (11689kb,D)

Title: Efficient and Scalable Chinese Vector Font Generation via Component
  Composition
Authors: Jinyu Song, Weitao You, Shuhui Shi, Shuxuan Guo, Lingyun Sun and Wei
  Wang
Categories: cs.CV cs.GR
Comments: 15 pages, 23 figures
\\
  Chinese vector font generation is challenging due to the complex structure
and huge amount of Chinese characters. Recent advances remain limited to
generating a small set of characters with simple structure. In this work, we
first observe that most Chinese characters can be disassembled into
frequently-reused components. Therefore, we introduce the first efficient and
scalable Chinese vector font generation approach via component composition,
allowing generating numerous vector characters from a small set of components.
To achieve this, we collect a large-scale dataset that contains over
\textit{90K} Chinese characters with their components and layout information.
Upon the dataset, we propose a simple yet effective framework based on spatial
transformer networks (STN) and multiple losses tailored to font characteristics
to learn the affine transformation of the components, which can be directly
applied to the B\'ezier curves, resulting in Chinese characters in vector
format. Our qualitative and quantitative experiments have demonstrated that our
method significantly surpasses the state-of-the-art vector font generation
methods in generating large-scale complex Chinese characters in both font
generation and zero-shot font extension.
\\ ( https://arxiv.org/abs/2404.06779 ,  11689kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06780
Date: Wed, 10 Apr 2024 06:41:30 GMT   (17068kb,D)

Title: Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior
Authors: Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, Changjun Jiang
Categories: cs.CV
Comments: Project page: https://urbanarchitect.github.io/
\\
  Text-to-3D generation has achieved remarkable success via large-scale
text-to-image diffusion models. Nevertheless, there is no paradigm for scaling
up the methodology to urban scale. Urban scenes, characterized by numerous
elements, intricate arrangement relationships, and vast scale, present a
formidable barrier to the interpretability of ambiguous textual descriptions
for effective model optimization. In this work, we surmount the limitations by
introducing a compositional 3D layout representation into text-to-3D paradigm,
serving as an additional prior. It comprises a set of semantic primitives with
simple geometric structures and explicit arrangement relationships,
complementing textual descriptions and enabling steerable generation. Upon
this, we propose two modifications -- (1) We introduce Layout-Guided
Variational Score Distillation to address model optimization inadequacies. It
conditions the score distillation sampling process with geometric and semantic
constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes,
we represent 3D scene with a Scalable Hash Grid structure, incrementally
adapting to the growing scale of urban scenes. Extensive experiments
substantiate the capability of our framework to scale text-to-3D generation to
large-scale urban scenes that cover over 1000m driving distance for the first
time. We also present various scene editing demonstrations, showing the powers
of steerable urban scene generation. Website: https://urbanarchitect.github.io.
\\ ( https://arxiv.org/abs/2404.06780 ,  17068kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06798
Date: Wed, 10 Apr 2024 07:41:35 GMT   (5130kb,D)

Title: MedRG: Medical Report Grounding with Multi-modal Large Language Model
Authors: Ke Zou and Yang Bai and Zhihao Chen and Yang Zhou and Yidi Chen and
  Kai Ren and Meng Wang and Xuedong Yuan and Xiaojing Shen and Huazhu Fu
Categories: cs.CV
Comments: 12 pages, 4 figures
\\
  Medical Report Grounding is pivotal in identifying the most relevant regions
in medical images based on a given phrase query, a critical aspect in medical
image analysis and radiological diagnosis. However, prevailing visual grounding
approaches necessitate the manual extraction of key phrases from medical
reports, imposing substantial burdens on both system efficiency and physicians.
In this paper, we introduce a novel framework, Medical Report Grounding
(MedRG), an end-to-end solution for utilizing a multi-modal Large Language
Model to predict key phrase by incorporating a unique token, BOX, into the
vocabulary to serve as an embedding for unlocking detection capabilities.
Subsequently, the vision encoder-decoder jointly decodes the hidden embedding
and the input medical image, generating the corresponding grounding box. The
experimental results validate the effectiveness of MedRG, surpassing the
performance of the existing state-of-the-art medical phrase grounding methods.
This study represents a pioneering exploration of the medical report grounding
task, marking the first-ever endeavor in this domain.
\\ ( https://arxiv.org/abs/2404.06798 ,  5130kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06814
Date: Wed, 10 Apr 2024 08:02:17 GMT   (15898kb,D)

Title: Zero-shot Point Cloud Completion Via 2D Priors
Authors: Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee
Categories: cs.CV
\\
  3D point cloud completion is designed to recover complete shapes from
partially observed point clouds. Conventional completion methods typically
depend on extensive point cloud data for training %, with their effectiveness
often constrained to object categories similar to those seen during training.
In contrast, we propose a zero-shot framework aimed at completing partially
observed point clouds across any unseen categories. Leveraging point rendering
via Gaussian Splatting, we develop techniques of Point Cloud Colorization and
Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion
models to infer missing regions. Experimental results on both synthetic and
real-world scanned point clouds demonstrate that our approach outperforms
existing methods in completing a variety of objects without any requirement for
specific training data.
\\ ( https://arxiv.org/abs/2404.06814 ,  15898kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06832
Date: Wed, 10 Apr 2024 08:48:09 GMT   (7769kb,D)

Title: SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection
Authors: Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn
Categories: cs.CV cs.LG
Comments: Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024
\\
  Detecting anomalies in images has become a well-explored problem in both
academia and industry. State-of-the-art algorithms are able to detect defects
in increasingly difficult settings and data modalities. However, most current
methods are not suited to address 3D objects captured from differing poses.
While solutions using Neural Radiance Fields (NeRFs) have been proposed, they
suffer from excessive computation requirements, which hinder real-world
usability. For this reason, we propose the novel 3D Gaussian splatting-based
framework SplatPose which, given multi-view images of a 3D object, accurately
estimates the pose of unseen views in a differentiable manner, and detects
anomalies in them. We achieve state-of-the-art results in both training and
inference speed, and detection performance, even when using less training data
than competing methods. We thoroughly evaluate our framework using the recently
proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly
detection (MAD) data set.
\\ ( https://arxiv.org/abs/2404.06832 ,  7769kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06835
Date: Wed, 10 Apr 2024 08:54:00 GMT   (7416kb,D)

Title: Tuning-Free Adaptive Style Incorporation for Structure-Consistent
  Text-Driven Style Transfer
Authors: Yanqi Ge, Jiaqi Liu, Qingnan Fan, Xi Jiang, Ye Huang, Shuai Qin, Hong
  Gu, Wen Li, Lixin Duan
Categories: cs.CV
\\
  In this work, we target the task of text-driven style transfer in the context
of text-to-image (T2I) diffusion models. The main challenge is consistent
structure preservation while enabling effective style transfer effects. The
past approaches in this field directly concatenate the content and style
prompts for a prompt-level style injection, leading to unavoidable structure
distortions. In this work, we propose a novel solution to the text-driven style
transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve
fine-grained feature-level style incorporation. It consists of the Siamese
Cross-Attention~(SiCA) to decouple the single-track cross-attention to a
dual-track structure to obtain separate content and style features, and the
Adaptive Content-Style Blending (AdaBlending) module to couple the content and
style information from a structure-consistent manner. Experimentally, our
method exhibits much better performance in both structure preservation and
stylized effects.
\\ ( https://arxiv.org/abs/2404.06835 ,  7416kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06836
Date: Wed, 10 Apr 2024 08:54:43 GMT   (3934kb,D)

Title: O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit
  Representation
Authors: Muer Tie, Julong Wei, Zhengjun Wang, Ke Wu, Shansuai Yuan, Kaizhao
  Zhang, Jie Jia, Jieru Zhao, Zhongxue Gan, Wenchao Ding
Categories: cs.CV
\\
  Online construction of open-ended language scenes is crucial for robotic
applications, where open-vocabulary interactive scene understanding is
required. Recently, neural implicit representation has provided a promising
direction for online interactive mapping. However, implementing open-vocabulary
scene understanding capability into online neural implicit mapping still faces
three challenges: lack of local scene updating ability, blurry spatial
hierarchical semantic segmentation and difficulty in maintaining multi-view
consistency. To this end, we proposed O2V-mapping, which utilizes voxel-based
language and geometric features to create an open-vocabulary field, thus
allowing for local updates during online training process. Additionally, we
leverage a foundational model for image segmentation to extract language
features on object-level entities, achieving clear segmentation boundaries and
hierarchical semantic features. For the purpose of preserving consistency in 3D
object properties across different viewpoints, we propose a spatial adaptive
voxel adjustment mechanism and a multi-view weight selection method. Extensive
experiments on open-vocabulary object localization and semantic segmentation
demonstrate that O2V-mapping achieves online construction of language scenes
while enhancing accuracy, outperforming the previous SOTA method.
\\ ( https://arxiv.org/abs/2404.06836 ,  3934kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06842
Date: Wed, 10 Apr 2024 09:14:28 GMT   (3519kb,D)

Title: MoCha-Stereo: Motif Channel Attention Network for Stereo Matching
Authors: Ziyang Chen and Wei Long and He Yao and Yongjun Zhang and Bingshu Wang
  and Yongbin Qin and Jia Wu
Categories: cs.CV
Comments: Accepted to CVPR 2024
Journal-ref: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2024
\\
  Learning-based stereo matching techniques have made significant progress.
However, existing methods inevitably lose geometrical structure information
during the feature channel generation process, resulting in edge detail
mismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network
(MoCha-Stereo) is designed to address this problem. We provide the Motif
Channel Correlation Volume (MCCV) to determine more accurate edge matching
costs. MCCV is achieved by projecting motif channels, which capture common
geometric structures in feature channels, onto feature maps and cost volumes.
In addition, edge variations in %potential feature channels of the
reconstruction error map also affect details matching, we propose the
Reconstruction Error Motif Penalty (REMP) module to further refine the
full-resolution disparity estimation. REMP integrates the frequency information
of typical channel features from the reconstruction error. MoCha-Stereo ranks
1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure
also shows excellent performance in Multi-View Stereo. Code is avaliable at
https://github.com/ZYangChen/MoCha-Stereo.
\\ ( https://arxiv.org/abs/2404.06842 ,  3519kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06851
Date: Wed, 10 Apr 2024 09:24:54 GMT   (26696kb,D)

Title: UDiFF: Generating Conditional Unsigned Distance Fields with Optimal
  Wavelet Diffusion
Authors: Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu,
  Zhizhong Han
Categories: cs.CV
Comments: To appear at CVPR2024. Project page:
  https://weiqi-zhang.github.io/UDiFF
\\
  Diffusion models have shown remarkable results for image generation, editing
and inpainting. Recent works explore diffusion models for 3D shape generation
with neural implicit functions, i.e., signed distance function and occupancy
function. However, they are limited to shapes with closed surfaces, which
prevents them from generating diverse 3D real-world contents containing open
surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned
distance fields (UDFs) which is capable to generate textured 3D shapes with
open surfaces from text conditions or unconditionally. Our key idea is to
generate UDFs in spatial-frequency domain with an optimal wavelet
transformation, which produces a compact representation space for UDF
generation. Specifically, instead of selecting an appropriate wavelet
transformation which requires expensive manual efforts and still leads to large
information loss, we propose a data-driven approach to learn the optimal
wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by
numerical and visual comparisons with the latest methods on widely used
benchmarks. Page: https://weiqi-zhang.github.io/UDiFF.
\\ ( https://arxiv.org/abs/2404.06851 ,  26696kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06859
Date: Wed, 10 Apr 2024 09:35:36 GMT   (993kb,D)

Title: Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark
Authors: Marina Ceccon, Davide Dalle Pezze, Alessandro Fabris, Gian Antonio
  Susto
Categories: cs.CV cs.AI
\\
  Multi-label image classification in dynamic environments is a problem that
poses significant challenges. Previous studies have primarily focused on
scenarios such as Domain Incremental Learning and Class Incremental Learning,
which do not fully capture the complexity of real-world applications. In this
paper, we study the problem of classification of medical imaging in the
scenario termed New Instances \& New Classes, which combines the challenges of
both new class arrivals and domain shifts in a single framework. Unlike
traditional scenarios, it reflects the realistic nature of CL in domains such
as medical imaging, where updates may introduce both new classes and changes in
domain characteristics. To address the unique challenges posed by this complex
scenario, we introduce a novel approach called Pseudo-Label Replay. This method
aims to mitigate forgetting while adapting to new classes and domain shifts by
combining the advantages of the Replay and Pseudo-Label methods and solving
their limitations in the proposed scenario. % part3 We evaluate our proposed
approach on a challenging benchmark consisting of two datasets, seven tasks,
and nineteen classes, modeling a realistic Continual Learning scenario. Our
experimental findings demonstrate the effectiveness of Pseudo-Label Replay in
addressing the challenges posed by the complex scenario proposed. Our method
surpasses existing approaches, exhibiting superior performance while showing
minimal forgetting.
\\ ( https://arxiv.org/abs/2404.06859 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06860
Date: Wed, 10 Apr 2024 09:35:50 GMT   (27165kb,D)

Title: Monocular 3D lane detection for Autonomous Driving: Recent Achievements,
  Challenges, and Outlooks
Authors: Fulong Ma, Weiqing Qi, Guoyang Zhao, Linwei Zheng, Sheng Wang and Ming
  Liu
Categories: cs.CV
\\
  3D lane detection plays a crucial role in autonomous driving by extracting
structural and traffic information from the road in 3D space to assist the
self-driving car in rational, safe, and comfortable path planning and motion
control. Due to the consideration of sensor costs and the advantages of visual
data in color information, in practical applications, 3D lane detection based
on monocular vision is one of the important research directions in the field of
autonomous driving, which has attracted more and more attention in both
industry and academia. Unfortunately, recent progress in visual perception
seems insufficient to develop completely reliable 3D lane detection algorithms,
which also hinders the development of vision-based fully autonomous
self-driving cars, i.e., achieving level 5 autonomous driving, driving like
human-controlled cars. This is one of the conclusions drawn from this review
paper: there is still a lot of room for improvement and significant
improvements are still needed in the 3D lane detection algorithm for autonomous
driving cars using visual sensors. Motivated by this, this review defines,
analyzes, and reviews the current achievements in the field of 3D lane
detection research, and the vast majority of the current progress relies
heavily on computationally complex deep learning models. In addition, this
review covers the 3D lane detection pipeline, investigates the performance of
state-of-the-art algorithms, analyzes the time complexity of cutting-edge
modeling choices, and highlights the main achievements and limitations of
current research efforts. The survey also includes a comprehensive discussion
of available 3D lane detection datasets and the challenges that researchers
have faced but have not yet resolved. Finally, our work outlines future
research directions and welcomes researchers and practitioners to enter this
exciting field.
\\ ( https://arxiv.org/abs/2404.06860 ,  27165kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06863
Date: Wed, 10 Apr 2024 09:40:56 GMT   (17730kb,D)

Title: RESSCAL3D: Resolution Scalable 3D Semantic Segmentation of Point Clouds
Authors: Remco Royen and Adrian Munteanu
Categories: cs.CV
Comments: Published at 2023 IEEE International Conference on Image Processing
  (ICIP)
DOI: 10.1109/ICIP49359.2023.10222338
\\
  While deep learning-based methods have demonstrated outstanding results in
numerous domains, some important functionalities are missing. Resolution
scalability is one of them. In this work, we introduce a novel architecture,
dubbed RESSCAL3D, providing resolution-scalable 3D semantic segmentation of
point clouds. In contrast to existing works, the proposed method does not
require the whole point cloud to be available to start inference. Once a
low-resolution version of the input point cloud is available, first semantic
predictions can be generated in an extremely fast manner. This enables early
decision-making in subsequent processing steps. As additional points become
available, these are processed in parallel. To improve performance, features
from previously computed scales are employed as prior knowledge at the current
scale. Our experiments show that RESSCAL3D is 31-62% faster than the
non-scalable baseline while keeping a limited impact on performance. To the
best of our knowledge, the proposed method is the first to propose a
resolution-scalable approach for 3D semantic segmentation of point clouds based
on deep learning.
\\ ( https://arxiv.org/abs/2404.06863 ,  17730kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06865
Date: Wed, 10 Apr 2024 09:45:02 GMT   (6935kb,D)

Title: Fine color guidance in diffusion models and its application to image
  compression at extremely low bitrates
Authors: Tom Bordin, Thomas Maugey
Categories: cs.CV
Comments: Submitted to IEEE Transactions on Image Processing (TIP)
\\
  This study addresses the challenge of, without training or fine-tuning,
controlling the global color aspect of images generated with a diffusion model.
We rewrite the guidance equations to ensure that the outputs are closer to a
known color map, and this without hindering the quality of the generation. Our
method leads to new guidance equations. We show in the color guidance context
that, the scaling of the guidance should not decrease but remains high
throughout the diffusion process. In a second contribution, our guidance is
applied in a compression framework, we combine both semantic and general color
information on the image to decode the images at low cost. We show that our
method is effective at improving fidelity and realism of compressed images at
extremely low bit rates, when compared to other classical or more semantic
oriented approaches.
\\ ( https://arxiv.org/abs/2404.06865 ,  6935kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06883
Date: Wed, 10 Apr 2024 10:13:37 GMT   (775kb)

Title: Research on Detection of Floating Objects in River and Lake Based on AI
  Intelligent Image Recognition
Authors: Jingyu Zhang, Ao Xiang, Yu Cheng, Qin Yang, Liyang Wang
Categories: cs.CV cs.AI
\\
  With the rapid advancement of artificial intelligence technology, AI-enabled
image recognition has emerged as a potent tool for addressing challenges in
traditional environmental monitoring. This study focuses on the detection of
floating objects in river and lake environments, exploring an innovative
approach based on deep learning. By intricately analyzing the technical
pathways for detecting static and dynamic features and considering the
characteristics of river and lake debris, a comprehensive image acquisition and
processing workflow has been developed. The study highlights the application
and performance comparison of three mainstream deep learning models -SSD,
Faster-RCNN, and YOLOv5- in debris identification. Additionally, a detection
system for floating objects has been designed and implemented, encompassing
both hardware platform construction and software framework development. Through
rigorous experimental validation, the proposed system has demonstrated its
ability to significantly enhance the accuracy and efficiency of debris
detection, thus offering a new technological avenue for water quality
monitoring in rivers and lakes
\\ ( https://arxiv.org/abs/2404.06883 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06892
Date: Wed, 10 Apr 2024 10:34:34 GMT   (23942kb,D)

Title: SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End
  Autonomous Driving
Authors: Diankun Zhang, Guoan Wang, Runwen Zhu, Jianbo Zhao, Xiwu Chen, Siyu
  Zhang, Jiahao Gong, Qibin Zhou, Wenyuan Zhang, Ningzi Wang, Feiyang Tan,
  Hangning Zhou, Ziyao Xu, Haotian Yao, Chi Zhang, Xiaojun Liu, Xiaoguang Di,
  and Bin Li
Categories: cs.CV
\\
  End-to-End paradigms use a unified framework to implement multi-tasks in an
autonomous driving system. Despite simplicity and clarity, the performance of
end-to-end autonomous driving methods on sub-tasks is still far behind the
single-task methods. Meanwhile, the widely used dense BEV features in previous
end-to-end methods make it costly to extend to more modalities or tasks. In
this paper, we propose a Sparse query-centric paradigm for end-to-end
Autonomous Driving (SparseAD), where the sparse queries completely represent
the whole driving scenario across space, time and tasks without any dense BEV
representation. Concretely, we design a unified sparse architecture for
perception tasks including detection, tracking, and online mapping. Moreover,
we revisit motion prediction and planning, and devise a more justifiable motion
planner framework. On the challenging nuScenes dataset, SparseAD achieves SOTA
full-task performance among end-to-end methods and significantly narrows the
performance gap between end-to-end paradigms and single-task methods. Codes
will be released soon.
\\ ( https://arxiv.org/abs/2404.06892 ,  23942kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06894
Date: Wed, 10 Apr 2024 10:36:15 GMT   (1130kb,D)

Title: O-TALC: Steps Towards Combating Oversegmentation within Online Action
  Segmentation
Authors: Matthew Kent Myers, Nick Wright, A. Stephen McGough, Nicholas Martin
Categories: cs.CV
Comments: 5 pages, 3 figures. Accepted as a short (unindexed) paper at the
  TAHRI conference
\\
  Online temporal action segmentation shows a strong potential to facilitate
many HRI tasks where extended human action sequences must be tracked and
understood in real time. Traditional action segmentation approaches, however,
operate in an offline two stage approach, relying on computationally expensive
video wide features for segmentation, rendering them unsuitable for online HRI
applications. In order to facilitate online action segmentation on a stream of
incoming video data, we introduce two methods for improved training and
inference of backbone action recognition models, allowing them to be deployed
directly for online frame level classification. Firstly, we introduce surround
dense sampling whilst training to facilitate training vs. inference clip
matching and improve segment boundary predictions. Secondly, we introduce an
Online Temporally Aware Label Cleaning (O-TALC) strategy to explicitly reduce
oversegmentation during online inference. As our methods are backbone
invariant, they can be deployed with computationally efficient spatio-temporal
action recognition models capable of operating in real time with a small
segmentation latency. We show our method outperforms similar online action
segmentation work as well as matches the performance of many offline models
with access to full temporal resolution when operating on challenging
fine-grained datasets.
\\ ( https://arxiv.org/abs/2404.06894 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06903
Date: Wed, 10 Apr 2024 10:46:59 GMT   (43040kb,D)

Title: DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic
  Gaussian Splatting
Authors: Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari,
  Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi
Categories: cs.CV cs.AI
\\
  The increasing demand for virtual reality applications has highlighted the
significance of crafting immersive 3D assets. We present a text-to-3D
360$^{\circ}$ scene generation pipeline that facilitates the creation of
comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of
minutes. Our approach utilizes the generative power of a 2D diffusion model and
prompt self-refinement to create a high-quality and globally coherent panoramic
image. This image acts as a preliminary "flat" (2D) scene representation.
Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to
enable real-time exploration. To produce consistent 3D geometry, our pipeline
constructs a spatially coherent structure by aligning the 2D monocular depth
into a globally optimized point cloud. This point cloud serves as the initial
state for the centroids of 3D Gaussians. In order to address invisible issues
inherent in single-view inputs, we impose semantic and geometric constraints on
both synthesized and input camera views as regularizations. These guide the
optimization of Gaussians, aiding in the reconstruction of unseen regions. In
summary, our method offers a globally consistent 3D scene within a
360$^{\circ}$ perspective, providing an enhanced immersive experience over
existing techniques. Project website at: http://dreamscene360.github.io/
\\ ( https://arxiv.org/abs/2404.06903 ,  43040kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06913
Date: Wed, 10 Apr 2024 11:06:29 GMT   (9238kb,D)

Title: Sparse Global Matching for Video Frame Interpolation with Large Motion
Authors: Chunxu Liu, Guozhen Zhang, Rui Zhao, Limin Wang
Categories: cs.CV
Comments: Accepted by CVPR 2024. Project page: https://sgm-vfi.github.io/
\\
  Large motion poses a critical challenge in Video Frame Interpolation (VFI)
task. Existing methods are often constrained by limited receptive fields,
resulting in sub-optimal performance when handling scenarios with large motion.
In this paper, we introduce a new pipeline for VFI, which can effectively
integrate global-level information to alleviate issues associated with large
motion. Specifically, we first estimate a pair of initial intermediate flows
using a high-resolution feature map for extracting local details. Then, we
incorporate a sparse global matching branch to compensate for flow estimation,
which consists of identifying flaws in initial flows and generating sparse flow
compensation with a global receptive field. Finally, we adaptively merge the
initial flow estimation with global flow compensation, yielding a more accurate
intermediate flow. To evaluate the effectiveness of our method in handling
large motion, we carefully curate a more challenging subset from commonly used
benchmarks. Our method demonstrates the state-of-the-art performance on these
VFI subsets with large motion.
\\ ( https://arxiv.org/abs/2404.06913 ,  9238kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06918
Date: Wed, 10 Apr 2024 11:10:50 GMT   (8629kb,D)

Title: HRVDA: High-Resolution Visual Document Assistant
Authors: Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong Liu,
  Deqiang Jiang, Xing Sun, Linli Xu
Categories: cs.CV
Comments: Accepted to CVPR 2024 main conference
\\
  Leveraging vast training data, multimodal large language models (MLLMs) have
demonstrated formidable general visual comprehension capabilities and achieved
remarkable performance across various tasks. However, their performance in
visual document understanding still leaves much room for improvement. This
discrepancy is primarily attributed to the fact that visual document
understanding is a fine-grained prediction task. In natural scenes, MLLMs
typically use low-resolution images, leading to a substantial loss of visual
information. Furthermore, general-purpose MLLMs do not excel in handling
document-oriented instructions. In this paper, we propose a High-Resolution
Visual Document Assistant (HRVDA), which bridges the gap between MLLMs and
visual document understanding. This model employs a content filtering mechanism
and an instruction filtering module to separately filter out the
content-agnostic visual tokens and instruction-agnostic visual tokens, thereby
achieving efficient model training and inference for high-resolution images. In
addition, we construct a document-oriented visual instruction tuning dataset
and apply a multi-stage training strategy to enhance the model's document
modeling capabilities. Extensive experiments demonstrate that our model
achieves state-of-the-art performance across multiple document understanding
datasets, while maintaining training efficiency and inference speed comparable
to low-resolution models.
\\ ( https://arxiv.org/abs/2404.06918 ,  8629kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06936
Date: Wed, 10 Apr 2024 11:40:02 GMT   (11576kb,D)

Title: Efficient and Generic Point Model for Lossless Point Cloud Attribute
  Compression
Authors: Kang You, Pan Gao, Zhan Ma
Categories: cs.CV cs.MM
\\
  The past several years have witnessed the emergence of learned point cloud
compression (PCC) techniques. However, current learning-based lossless point
cloud attribute compression (PCAC) methods either suffer from high
computational complexity or deteriorated compression performance. Moreover, the
significant variations in point cloud scale and sparsity encountered in
real-world applications make developing an all-in-one neural model a
challenging task. In this paper, we propose PoLoPCAC, an efficient and generic
lossless PCAC method that achieves high compression efficiency and strong
generalizability simultaneously. We formulate lossless PCAC as the task of
inferring explicit distributions of attributes from group-wise autoregressive
priors. A progressive random grouping strategy is first devised to efficiently
resolve the point cloud into groups, and then the attributes of each group are
modeled sequentially from accumulated antecedents. A locality-aware attention
mechanism is utilized to exploit prior knowledge from context windows in
parallel. Since our method directly operates on points, it can naturally avoids
distortion caused by voxelization, and can be executed on point clouds with
arbitrary scale and density. Experiments show that our method can be instantly
deployed once trained on a Synthetic 2k-ShapeNet dataset while enjoying
continuous bit-rate reduction over the latest G-PCCv23 on various datasets
(ShapeNet, ScanNet, MVUB, 8iVFB). Meanwhile, our method reports shorter coding
time than G-PCCv23 on the majority of sequences with a lightweight model size
(2.6MB), which is highly attractive for practical applications. Dataset, code
and trained model are available at
https://github.com/I2-Multimedia-Lab/PoLoPCAC.
\\ ( https://arxiv.org/abs/2404.06936 ,  11576kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06957
Date: Wed, 10 Apr 2024 12:17:25 GMT   (13481kb,D)

Title: Adversarial purification for no-reference image-quality metrics:
  applicability study and new methods
Authors: Aleksandr Gushchin, Anna Chistyakova, Vladislav Minashkin, Anastasia
  Antsiferova and Dmitriy Vatolin
Categories: cs.CV cs.AI
\\
  Recently, the area of adversarial attacks on image quality metrics has begun
to be explored, whereas the area of defences remains under-researched. In this
study, we aim to cover that case and check the transferability of adversarial
purification defences from image classifiers to IQA methods. In this paper, we
apply several widespread attacks on IQA models and examine the success of the
defences against them. The purification methodologies covered different
preprocessing techniques, including geometrical transformations, compression,
denoising, and modern neural network-based methods. Also, we address the
challenge of assessing the efficacy of a defensive methodology by proposing
ways to estimate output visual quality and the success of neutralizing attacks.
Defences were tested against attack on three IQA metrics -- Linearity, MetaIQA
and SPAQ. The code for attacks and defences is available at: (link is hidden
for a blind review).
\\ ( https://arxiv.org/abs/2404.06957 ,  13481kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06963
Date: Wed, 10 Apr 2024 12:22:19 GMT   (761kb,D)

Title: V-MAD: Video-based Morphing Attack Detection in Operational Scenarios
Authors: Guido Borghi, Annalisa Franco, Nicol\`o Di Domenico, Matteo Ferrara,
  Davide Maltoni
Categories: cs.CV
\\
  In response to the rising threat of the face morphing attack, this paper
introduces and explores the potential of Video-based Morphing Attack Detection
(V-MAD) systems in real-world operational scenarios. While current morphing
attack detection methods primarily focus on a single or a pair of images, V-MAD
is based on video sequences, exploiting the video streams often acquired by
face verification tools available, for instance, at airport gates. Through this
study, we show for the first time the advantages that the availability of
multiple probe frames can bring to the morphing attack detection task,
especially in scenarios where the quality of probe images is varied and might
be affected, for instance, by pose or illumination variations. Experimental
results on a real operational database demonstrate that video sequences
represent valuable information for increasing the robustness and performance of
morphing attack detection systems.
\\ ( https://arxiv.org/abs/2404.06963 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06971
Date: Wed, 10 Apr 2024 12:31:43 GMT   (3044kb,D)

Title: TrajPRed: Trajectory Prediction with Region-based Relation Learning
Authors: Chen Zhou, Ghassan AlRegib, Armin Parchami, Kunjan Singh
Categories: cs.CV cs.AI cs.LG
DOI: 10.1109/TITS.2024.3381843
\\
  Forecasting human trajectories in traffic scenes is critical for safety
within mixed or fully autonomous systems. Human future trajectories are driven
by two major stimuli, social interactions, and stochastic goals. Thus, reliable
forecasting needs to capture these two stimuli. Edge-based relation modeling
represents social interactions using pairwise correlations from precise
individual states. Nevertheless, edge-based relations can be vulnerable under
perturbations. To alleviate these issues, we propose a region-based relation
learning paradigm that models social interactions via region-wise dynamics of
joint states, i.e., the changes in the density of crowds. In particular,
region-wise agent joint information is encoded within convolutional feature
grids. Social relations are modeled by relating the temporal changes of local
joint information from a global perspective. We show that region-based
relations are less susceptible to perturbations. In order to account for the
stochastic individual goals, we exploit a conditional variational autoencoder
to realize multi-goal estimation and diverse future prediction. Specifically,
we perform variational inference via the latent distribution, which is
conditioned on the correlation between input states and associated target
goals. Sampling from the latent distribution enables the framework to reliably
capture the stochastic behavior in test data. We integrate multi-goal
estimation and region-based relation learning to model the two stimuli, social
interactions, and stochastic goals, in a prediction framework. We evaluate our
framework on the ETH-UCY dataset and Stanford Drone Dataset (SDD). We show that
the diverse prediction better fits the ground truth when incorporating the
relation module. Our framework outperforms the state-of-the-art models on SDD
by $27.61\%$/$18.20\%$ of ADE/FDE metrics.
\\ ( https://arxiv.org/abs/2404.06971 ,  3044kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06977
Date: Wed, 10 Apr 2024 12:45:27 GMT   (1715kb)

Title: Accurate Tennis Court Line Detection on Amateur Recorded Matches
Authors: Sameer Agrawal, Ragoth Sundararajan and Vishak Sagar
Categories: cs.CV
Comments: Accepted to 5th International conference on Image, Video Processing
  and Artificial Intelligence
ACM-class: I.4.6
\\
  Typically, tennis court line detection is done by running
Hough-Line-Detection to find straight lines in the image, and then computing a
transformation matrix from the detected lines to create the final court
structure. We propose numerous improvements and enhancements to this algorithm,
including using pretrained State-of-the-Art shadow-removal and object-detection
ML models to make our line-detection more robust. Compared to the original
algorithm, our method can accurately detect lines on amateur, dirty courts.
When combined with a robust ball-tracking system, our method will enable
accurate, automatic refereeing for amateur and professional tennis matches
alike.
\\ ( https://arxiv.org/abs/2404.06977 ,  1715kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07029
Date: Wed, 10 Apr 2024 14:22:16 GMT   (2274kb,D)

Title: Diffusion-based inpainting of incomplete Euclidean distance matrices of
  trajectories generated by a fractional Brownian motion
Authors: Alexander Lobashev, Kirill Polovnikov
Categories: cs.CV
MSC-class: 68T07
ACM-class: I.2.0
\\
  Fractional Brownian trajectories (fBm) feature both randomness and strong
scale-free correlations, challenging generative models to reproduce the
intrinsic memory characterizing the underlying process. Here we test a
diffusion probabilistic model on a specific dataset of corrupted images
corresponding to incomplete Euclidean distance matrices of fBm at various
memory exponents $H$. Our dataset implies uniqueness of the data imputation in
the regime of low missing ratio, where the remaining partial graph is rigid,
providing the ground truth for the inpainting. We find that the conditional
diffusion generation stably reproduces the statistics of missing
fBm-distributed distances for different values of $H$ exponent. Furthermore,
while diffusion models have been recently shown to remember samples from the
training database, we show that diffusion-based inpainting behaves
qualitatively different from the database search with the increasing database
size. Finally, we apply our fBm-trained diffusion model with $H=1/3$ for
completion of chromosome distance matrices obtained in single-cell microscopy
experiments, showing its superiority over the standard bioinformatics
algorithms. Our source code is available on GitHub at
https://github.com/alobashev/diffusion_fbm.
\\ ( https://arxiv.org/abs/2404.07029 ,  2274kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07031
Date: Wed, 10 Apr 2024 14:24:10 GMT   (5379kb,D)

Title: ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR
  Domain Modeling
Authors: Ege \"Ozsoy, Chantal Pellegrini, Matthias Keicher, Nassir Navab
Categories: cs.CV
Comments: 11 pages, 3 figures, 7 tables
\\
  Every day, countless surgeries are performed worldwide, each within the
distinct settings of operating rooms (ORs) that vary not only in their setups
but also in the personnel, tools, and equipment used. This inherent diversity
poses a substantial challenge for achieving a holistic understanding of the OR,
as it requires models to generalize beyond their initial training datasets. To
reduce this gap, we introduce ORacle, an advanced vision-language model
designed for holistic OR domain modeling, which incorporates multi-view and
temporal capabilities and can leverage external knowledge during inference,
enabling it to adapt to previously unseen surgical scenarios. This capability
is further enhanced by our novel data augmentation framework, which
significantly diversifies the training dataset, ensuring ORacle's proficiency
in applying the provided knowledge effectively. In rigorous testing, in scene
graph generation, and downstream tasks on the 4D-OR dataset, ORacle not only
demonstrates state-of-the-art performance but does so requiring less data than
existing models. Furthermore, its adaptability is displayed through its ability
to interpret unseen views, actions, and appearances of tools and equipment.
This demonstrates ORacle's potential to significantly enhance the scalability
and affordability of OR domain modeling and opens a pathway for future
advancements in surgical data science. We will release our code and data upon
acceptance.
\\ ( https://arxiv.org/abs/2404.07031 ,  5379kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07032
Date: Wed, 10 Apr 2024 14:25:23 GMT   (2671kb,D)

Title: An Evidential-enhanced Tri-Branch Consistency Learning Method for
  Semi-supervised Medical Image Segmentation
Authors: Zhenxi Zhang, Heng Zhou, Xiaoran Shi, Ran Ran, Chunna Tian, Feng Zhou
Categories: cs.CV
\\
  Semi-supervised segmentation presents a promising approach for large-scale
medical image analysis, effectively reducing annotation burdens while achieving
comparable performance. This methodology holds substantial potential for
streamlining the segmentation process and enhancing its feasibility within
clinical settings for translational investigations. While cross-supervised
training, based on distinct co-training sub-networks, has become a prevalent
paradigm for this task, addressing critical issues such as predication
disagreement and label-noise suppression requires further attention and
progress in cross-supervised training. In this paper, we introduce an
Evidential Tri-Branch Consistency learning framework (ETC-Net) for
semi-supervised medical image segmentation. ETC-Net employs three branches: an
evidential conservative branch, an evidential progressive branch, and an
evidential fusion branch. The first two branches exhibit complementary
characteristics, allowing them to address prediction diversity and enhance
training stability. We also integrate uncertainty estimation from the
evidential learning into cross-supervised training, mitigating the negative
impact of erroneous supervision signals. Additionally, the evidential fusion
branch capitalizes on the complementary attributes of the first two branches
and leverages an evidence-based Dempster-Shafer fusion strategy, supervised by
more reliable and accurate pseudo-labels of unlabeled data. Extensive
experiments conducted on LA, Pancreas-CT, and ACDC datasets demonstrate that
ETC-Net surpasses other state-of-the-art methods for semi-supervised
segmentation. The code will be made available in the near future at
https://github.com/Medsemiseg.
\\ ( https://arxiv.org/abs/2404.07032 ,  2671kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07045
Date: Wed, 10 Apr 2024 14:35:22 GMT   (30727kb,D)

Title: Identification of Fine-grained Systematic Errors via Controlled Scene
  Generation
Authors: Valentyn Boreiko and Matthias Hein and Jan Hendrik Metzen
Categories: cs.CV
\\
  Many safety-critical applications, especially in autonomous driving, require
reliable object detectors. They can be very effectively assisted by a method to
search for and identify potential failures and systematic errors before these
detectors are deployed. Systematic errors are characterized by combinations of
attributes such as object location, scale, orientation, and color, as well as
the composition of their respective backgrounds. To identify them, one must
rely on something other than real images from a test set because they do not
account for very rare but possible combinations of attributes. To overcome this
limitation, we propose a pipeline for generating realistic synthetic scenes
with fine-grained control, allowing the creation of complex scenes with
multiple objects. Our approach, BEV2EGO, allows for a realistic generation of
the complete scene with road-contingent control that maps 2D bird's-eye view
(BEV) scene configurations to a first-person view (EGO). In addition, we
propose a benchmark for controlled scene generation to select the most
appropriate generative outpainting model for BEV2EGO. We further use it to
perform a systematic analysis of multiple state-of-the-art object detection
models and discover differences between them.
\\ ( https://arxiv.org/abs/2404.07045 ,  30727kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07072
Date: Wed, 10 Apr 2024 15:02:26 GMT   (1767kb,D)

Title: Implicit Multi-Spectral Transformer: An Lightweight and Effective
  Visible to Infrared Image Translation Model
Authors: Yijia Chen, Pinghua Chen, Xiangxin Zhou, Yingtie Lei, Ziyang Zhou,
  Mingxian Li
Categories: cs.CV
Comments: Accepted by IJCNN 2024
\\
  In the field of computer vision, visible light images often exhibit low
contrast in low-light conditions, presenting a significant challenge. While
infrared imagery provides a potential solution, its utilization entails high
costs and practical limitations. Recent advancements in deep learning,
particularly the deployment of Generative Adversarial Networks (GANs), have
facilitated the transformation of visible light images to infrared images.
However, these methods often experience unstable training phases and may
produce suboptimal outputs. To address these issues, we propose a novel
end-to-end Transformer-based model that efficiently converts visible light
images into high-fidelity infrared images. Initially, the Texture Mapping
Module and Color Perception Adapter collaborate to extract texture and color
features from the visible light image. The Dynamic Fusion Aggregation Module
subsequently integrates these features. Finally, the transformation into an
infrared image is refined through the synergistic action of the Color
Perception Adapter and the Enhanced Perception Attention mechanism.
Comprehensive benchmarking experiments confirm that our model outperforms
existing methods, producing infrared images of markedly superior quality, both
qualitatively and quantitatively. Furthermore, the proposed model enables more
effective downstream applications for infrared images than other methods.
\\ ( https://arxiv.org/abs/2404.07072 ,  1767kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07078
Date: Wed, 10 Apr 2024 15:09:15 GMT   (19764kb,D)

Title: VLLMs Provide Better Context for Emotion Understanding Through Common
  Sense Reasoning
Authors: Alexandros Xenos, Niki Maria Foteinopoulou, Ioanna Ntinou, Ioannis
  Patras, Georgios Tzimiropoulos
Categories: cs.CV cs.HC
Comments: A. Xenos, N. Foteinopoulou and I. Ntinou contributed equally to this
  work; 14 pages, 5 figures
\\
  Recognising emotions in context involves identifying the apparent emotions of
an individual, taking into account contextual cues from the surrounding scene.
Previous approaches to this task have involved the design of explicit
scene-encoding architectures or the incorporation of external scene-related
information, such as captions. However, these methods often utilise limited
contextual information or rely on intricate training pipelines. In this work,
we leverage the groundbreaking capabilities of Vision-and-Large-Language Models
(VLLMs) to enhance in-context emotion classification without introducing
complexity to the training process in a two-stage approach. In the first stage,
we propose prompting VLLMs to generate descriptions in natural language of the
subject's apparent emotion relative to the visual context. In the second stage,
the descriptions are used as contextual information and, along with the image
input, are used to train a transformer-based architecture that fuses text and
visual features before the final classification task. Our experimental results
show that the text and image features have complementary information, and our
fused architecture significantly outperforms the individual modalities without
any complex training methods. We evaluate our approach on three different
datasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art or
comparable accuracy across all datasets and metrics compared to much more
complex approaches. The code will be made publicly available on github:
https://github.com/NickyFot/EmoCommonSense.git
\\ ( https://arxiv.org/abs/2404.07078 ,  19764kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07094
Date: Wed, 10 Apr 2024 15:34:10 GMT   (3531kb,D)

Title: MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation
  from 2D Keypoints
Authors: Bedirhan Uguz, Ozhan Suat, Batuhan Karagoz, Emre Akbas
Categories: cs.CV
Comments: accepted to CVPRW 2024
\\
  This paper presents Key2Mesh, a model that takes a set of 2D human pose
keypoints as input and estimates the corresponding body mesh. Since this
process does not involve any visual (i.e. RGB image) data, the model can be
trained on large-scale motion capture (MoCap) datasets, thereby overcoming the
scarcity of image datasets with 3D labels. To enable the model's application on
RGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D
keypoints, and then feed these 2D keypoints to Key2Mesh. To improve the
performance of our model on RGB images, we apply an adversarial domain
adaptation (DA) method to bridge the gap between the MoCap and visual domains.
Crucially, our DA method does not require 3D labels for visual data, which
enables adaptation to target sets without the need for costly labels. We
evaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints,
in the absence of RGB and mesh label pairs. Our results on widely used H3.6M
and 3DPW datasets show that Key2Mesh sets the new state-of-the-art by
outperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE
for the 3DPW dataset. Thanks to our model's simple architecture, it operates at
least 12x faster than the prior state-of-the-art model, LGD. Additional
qualitative samples and code are available on the project website:
https://key2mesh.github.io/.
\\ ( https://arxiv.org/abs/2404.07094 ,  3531kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07097
Date: Wed, 10 Apr 2024 15:37:00 GMT   (3984kb,D)

Title: Learning Priors for Non Rigid SfM from Casual Videos
Authors: Yoni Kasten, Wuyue Lu, Haggai Maron
Categories: cs.CV
\\
  We tackle the long-standing challenge of reconstructing 3D structures and
camera positions from videos. The problem is particularly hard when objects are
transformed in a non-rigid way. Current approaches to this problem make
unrealistic assumptions or require a long optimization time.
  We present TracksTo4D, a novel deep learning-based approach that enables
inferring 3D structure and camera positions from dynamic content originating
from in-the-wild videos using a single feed-forward pass on a sparse point
track matrix. To achieve this, we leverage recent advances in 2D point tracking
and design an equivariant neural architecture tailored for directly processing
2D point tracks by leveraging their symmetries. TracksTo4D is trained on a
dataset of in-the-wild videos utilizing only the 2D point tracks extracted from
the videos, without any 3D supervision. Our experiments demonstrate that
TracksTo4D generalizes well to unseen videos of unseen semantic categories at
inference time, producing equivalent results to state-of-the-art methods while
significantly reducing the runtime compared to other baselines.
\\ ( https://arxiv.org/abs/2404.07097 ,  3984kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07106
Date: Wed, 10 Apr 2024 15:45:03 GMT   (4361kb,D)

Title: 3DMambaComplete: Exploring Structured State Space Model for Point Cloud
  Completion
Authors: Yixuan Li and Weidong Yang and Ben Fei
Categories: cs.CV cs.GR
Comments: 10 pages, 8 figures, 7 tables
\\
  Point cloud completion aims to generate a complete and high-fidelity point
cloud from an initially incomplete and low-quality input. A prevalent strategy
involves leveraging Transformer-based models to encode global features and
facilitate the reconstruction process. However, the adoption of pooling
operations to obtain global feature representations often results in the loss
of local details within the point cloud. Moreover, the attention mechanism
inherent in Transformers introduces additional computational complexity,
rendering it challenging to handle long sequences effectively. To address these
issues, we propose 3DMambaComplete, a point cloud completion network built on
the novel Mamba framework. It comprises three modules: HyperPoint Generation
encodes point cloud features using Mamba's selection mechanism and predicts a
set of Hyperpoints. A specific offset is estimated, and the down-sampled points
become HyperPoints. The HyperPoint Spread module disperses these HyperPoints
across different spatial locations to avoid concentration. Finally, a
deformation method transforms the 2D mesh representation of HyperPoints into a
fine-grained 3D structure for point cloud reconstruction. Extensive experiments
conducted on various established benchmarks demonstrate that 3DMambaComplete
surpasses state-of-the-art point cloud completion methods, as confirmed by
qualitative and quantitative analyses.
\\ ( https://arxiv.org/abs/2404.07106 ,  4361kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07112
Date: Wed, 10 Apr 2024 15:51:46 GMT   (1535kb,D)

Title: Unfolding ADMM for Enhanced Subspace Clustering of Hyperspectral Images
Authors: Xianlu Li, Nicolas Nadisic, Shaoguang Huang and Aleksandra
  Pi\v{z}urica
Categories: cs.CV
\\
  Deep subspace clustering methods are now prominent in clustering, typically
using fully connected networks and a self-representation loss function.
However, these methods often struggle with overfitting and lack
interpretability. In this paper, we explore an alternative clustering approach
based on deep unfolding. By unfolding iterative optimization methods into
neural networks, this approach offers enhanced interpretability and reliability
compared to data-driven deep learning methods, and greater adaptability and
generalization than model-based approaches. Hence, unfolding has become widely
used in inverse imaging problems, such as image restoration, reconstruction,
and super-resolution, but has not been sufficiently explored yet in the context
of clustering. In this work, we introduce an innovative clustering architecture
for hyperspectral images (HSI) by unfolding an iterative solver based on the
Alternating Direction Method of Multipliers (ADMM) for sparse subspace
clustering. To our knowledge, this is the first attempt to apply unfolding ADMM
for computing the self-representation matrix in subspace clustering. Moreover,
our approach captures well the structural characteristics of HSI data by
employing the K nearest neighbors algorithm as part of a structure preservation
module. Experimental evaluation of three established HSI datasets shows clearly
the potential of the unfolding approach in HSI clustering and even demonstrates
superior performance compared to state-of-the-art techniques.
\\ ( https://arxiv.org/abs/2404.07112 ,  1535kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07122
Date: Wed, 10 Apr 2024 16:01:37 GMT   (9315kb,D)

Title: Driver Attention Tracking and Analysis
Authors: Dat Viet Thanh Nguyen, Anh Tran, Nam Vu, Cuong Pham, Minh Hoai
Categories: cs.CV
\\
  We propose a novel method to estimate a driver's points-of-gaze using a pair
of ordinary cameras mounted on the windshield and dashboard of a car. This is a
challenging problem due to the dynamics of traffic environments with 3D scenes
of unknown depths. This problem is further complicated by the volatile distance
between the driver and the camera system. To tackle these challenges, we
develop a novel convolutional network that simultaneously analyzes the image of
the scene and the image of the driver's face. This network has a camera
calibration module that can compute an embedding vector that represents the
spatial configuration between the driver and the camera system. This
calibration module improves the overall network's performance, which can be
jointly trained end to end.
  We also address the lack of annotated data for training and evaluation by
introducing a large-scale driving dataset with point-of-gaze annotations. This
is an in situ dataset of real driving sessions in an urban city, containing
synchronized images of the driving scene as well as the face and gaze of the
driver. Experiments on this dataset show that the proposed method outperforms
various baseline methods, having the mean prediction error of 29.69 pixels,
which is relatively small compared to the $1280{\times}720$ resolution of the
scene camera.
\\ ( https://arxiv.org/abs/2404.07122 ,  9315kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07124
Date: Wed, 10 Apr 2024 16:04:21 GMT   (4270kb,D)

Title: Measuring proximity to standard planes during fetal brain ultrasound
  scanning
Authors: Chiara Di Vece, Antonio Cirigliano, Meala Le Lous, Raffaele
  Napolitano, Anna L. David, Donald Peebles, Pierre Jannin, Francisco
  Vasconcelos, Danail Stoyanov
Categories: cs.CV cs.AI
Comments: 11 pages, 5 figures
ACM-class: I.2.0; I.4.0; J.2.0; J.3.0
\\
  This paper introduces a novel pipeline designed to bring ultrasound (US)
plane pose estimation closer to clinical use for more effective navigation to
the standard planes (SPs) in the fetal brain. We propose a semi-supervised
segmentation model utilizing both labeled SPs and unlabeled 3D US volume
slices. Our model enables reliable segmentation across a diverse set of fetal
brain images. Furthermore, the model incorporates a classification mechanism to
identify the fetal brain precisely. Our model not only filters out frames
lacking the brain but also generates masks for those containing it, enhancing
the relevance of plane pose regression in clinical settings. We focus on fetal
brain navigation from 2D ultrasound (US) video analysis and combine this model
with a US plane pose regression network to provide sensorless proximity
detection to SPs and non-SPs planes; we emphasize the importance of proximity
detection to SPs for guiding sonographers, offering a substantial advantage
over traditional methods by allowing earlier and more precise adjustments
during scanning. We demonstrate the practical applicability of our approach
through validation on real fetal scan videos obtained from sonographers of
varying expertise levels. Our findings demonstrate the potential of our
approach to complement existing fetal US technologies and advance prenatal
diagnostic practices.
\\ ( https://arxiv.org/abs/2404.07124 ,  4270kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07153
Date: Wed, 10 Apr 2024 16:39:50 GMT   (43373kb,D)

Title: Lost in Translation: Modern Neural Networks Still Struggle With Small
  Realistic Image Transformations
Authors: Ofir Shifman and Yair Weiss
Categories: cs.CV
Comments: 14 pages, 6 appendices, 17 figures
\\
  Deep neural networks that achieve remarkable performance in image
classification have previously been shown to be easily fooled by tiny
transformations such as a one pixel translation of the input image. In order to
address this problem, two approaches have been proposed in recent years. The
first approach suggests using huge datasets together with data augmentation in
the hope that a highly varied training set will teach the network to learn to
be invariant. The second approach suggests using architectural modifications
based on sampling theory to deal explicitly with image translations. In this
paper, we show that these approaches still fall short in robustly handling
'natural' image translations that simulate a subtle change in camera
orientation. Our findings reveal that a mere one-pixel translation can result
in a significant change in the predicted image representation for approximately
40% of the test images in state-of-the-art models (e.g. open-CLIP trained on
LAION-2B or DINO-v2) , while models that are explicitly constructed to be
robust to cyclic translations can still be fooled with 1 pixel realistic
(non-cyclic) translations 11% of the time. We present Robust Inference by Crop
Selection: a simple method that can be proven to achieve any desired level of
consistency, although with a modest tradeoff with the model's accuracy.
Importantly, we demonstrate how employing this method reduces the ability to
fool state-of-the-art models with a 1 pixel translation to less than 5% while
suffering from only a 1% drop in classification accuracy. Additionally, we show
that our method can be easy adjusted to deal with circular shifts as well. In
such case we achieve 100% robustness to integer shifts with state-of-the-art
accuracy, and with no need for any further training.
\\ ( https://arxiv.org/abs/2404.07153 ,  43373kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07155
Date: Wed, 10 Apr 2024 16:44:11 GMT   (4617kb,D)

Title: Unified Language-driven Zero-shot Domain Adaptation
Authors: Senqiao Yang, Zhuotao Tian, Li Jiang, Jiaya Jia
Categories: cs.CV
Comments: Accepted by CVPR 2024
\\
  This paper introduces Unified Language-driven Zero-shot Domain Adaptation
(ULDA), a novel task setting that enables a single model to adapt to diverse
target domains without explicit domain-ID knowledge. We identify the
constraints in the existing language-driven zero-shot domain adaptation task,
particularly the requirement for domain IDs and domain-specific models, which
may restrict flexibility and scalability. To overcome these issues, we propose
a new framework for ULDA, consisting of Hierarchical Context Alignment (HCA),
Domain Consistent Representation Learning (DCRL), and Text-Driven Rectifier
(TDR). These components work synergistically to align simulated features with
target text across multiple visual levels, retain semantic correlations between
different regional representations, and rectify biases between simulated and
real target visual features, respectively. Our extensive empirical evaluations
demonstrate that this framework achieves competitive performance in both
settings, surpassing even the model that requires domain-ID, showcasing its
superiority and generalization ability. The proposed method is not only
effective but also maintains practicality and efficiency, as it does not
introduce additional computational costs during inference. Our project page is
https://senqiaoyang.com/project/ULDA .
\\ ( https://arxiv.org/abs/2404.07155 ,  4617kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07176
Date: Wed, 10 Apr 2024 17:25:42 GMT   (11316kb,D)

Title: Self-supervised Monocular Depth Estimation on Water Scenes via Specular
  Reflection Prior
Authors: Zhengyang Lu and Ying Chen
Categories: cs.CV
Comments: 16 pages, 8 figures
\\
  Monocular depth estimation from a single image is an ill-posed problem for
computer vision due to insufficient reliable cues as the prior knowledge.
Besides the inter-frame supervision, namely stereo and adjacent frames,
extensive prior information is available in the same frame. Reflections from
specular surfaces, informative intra-frame priors, enable us to reformulate the
ill-posed depth estimation task as a multi-view synthesis. This paper proposes
the first self-supervision for deep-learning depth estimation on water scenes
via intra-frame priors, known as reflection supervision and geometrical
constraints. In the first stage, a water segmentation network is performed to
separate the reflection components from the entire image. Next, we construct a
self-supervised framework to predict the target appearance from reflections,
perceived as other perspectives. The photometric re-projection error,
incorporating SmoothL1 and a novel photometric adaptive SSIM, is formulated to
optimize pose and depth estimation by aligning the transformed virtual depths
and source ones. As a supplement, the water surface is determined from real and
virtual camera positions, which complement the depth of the water area.
Furthermore, to alleviate these laborious ground truth annotations, we
introduce a large-scale water reflection scene (WRS) dataset rendered from
Unreal Engine 4. Extensive experiments on the WRS dataset prove the feasibility
of the proposed method compared to state-of-the-art depth estimation
techniques.
\\ ( https://arxiv.org/abs/2404.07176 ,  11316kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07178
Date: Wed, 10 Apr 2024 17:28:16 GMT   (22119kb,D)

Title: Move Anything with Layered Scene Diffusion
Authors: Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, Antoine
  Toisoul
Categories: cs.CV
Comments: CVPR 2024 camera-ready
\\
  Diffusion models generate images with an unprecedented level of quality, but
how can we freely rearrange image layouts? Recent works generate controllable
scenes via learning spatially disentangled latent codes, but these methods do
not apply to diffusion models due to their fixed forward process. In this work,
we propose SceneDiffusion to optimize a layered scene representation during the
diffusion sampling process. Our key insight is that spatial disentanglement can
be obtained by jointly denoising scene renderings at different spatial layouts.
Our generated scenes support a wide range of spatial editing operations,
including moving, resizing, cloning, and layer-wise appearance editing
operations, including object restyling and replacing. Moreover, a scene can be
generated conditioned on a reference image, thus enabling object moving for
in-the-wild images. Notably, this approach is training-free, compatible with
general text-to-image diffusion models, and responsive in less than a second.
\\ ( https://arxiv.org/abs/2404.07178 ,  22119kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07191
Date: Wed, 10 Apr 2024 17:48:37 GMT   (3848kb,D)

Title: InstantMesh: Efficient 3D Mesh Generation from a Single Image with
  Sparse-view Large Reconstruction Models
Authors: Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying
  Shan
Categories: cs.CV
Comments: Technical report. Project: https://github.com/TencentARC/InstantMesh
\\
  We present InstantMesh, a feed-forward framework for instant 3D mesh
generation from a single image, featuring state-of-the-art generation quality
and significant training scalability. By synergizing the strengths of an
off-the-shelf multiview diffusion model and a sparse-view reconstruction model
based on the LRM architecture, InstantMesh is able to create diverse 3D assets
within 10 seconds. To enhance the training efficiency and exploit more
geometric supervisions, e.g, depths and normals, we integrate a differentiable
iso-surface extraction module into our framework and directly optimize on the
mesh representation. Experimental results on public datasets demonstrate that
InstantMesh significantly outperforms other latest image-to-3D baselines, both
qualitatively and quantitatively. We release all the code, weights, and demo of
InstantMesh, with the intention that it can make substantial contributions to
the community of 3D generative AI and empower both researchers and content
creators.
\\ ( https://arxiv.org/abs/2404.07191 ,  3848kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07199
Date: Wed, 10 Apr 2024 17:57:41 GMT   (29478kb,D)

Title: RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth
  Diffusion
Authors: Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project Page: https://realmdreamer.github.io/
\\
  We introduce RealmDreamer, a technique for generation of general
forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D
Gaussian Splatting representation to match complex text prompts. We initialize
these splats by utilizing the state-of-the-art text-to-image generators,
lifting their samples into 3D, and computing the occlusion volume. We then
optimize this representation across multiple views as a 3D inpainting task with
image-conditional diffusion models. To learn correct geometric structure, we
incorporate a depth diffusion model by conditioning on the samples from the
inpainting model, giving rich geometric structure. Finally, we finetune the
model using sharpened samples from image generators. Notably, our technique
does not require video or multi-view data and can synthesize a variety of
high-quality 3D scenes in different styles, consisting of multiple objects. Its
generality additionally allows 3D synthesis from a single image.
\\ ( https://arxiv.org/abs/2404.07199 ,  29478kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07202
Date: Wed, 10 Apr 2024 17:59:20 GMT   (11163kb,D)

Title: UMBRAE: Unified Multimodal Decoding of Brain Signals
Authors: Weihao Xia, Raoul de Charette, Cengiz \"Oztireli, Jing-Hao Xue
Categories: cs.CV cs.AI cs.CL
Comments: Project Page: https://weihaox.github.io/UMBRAE
\\
  We address prevailing challenges of the brain-powered research, departing
from the observation that the literature hardly recover accurate spatial
information and require subject-specific models. To address these challenges,
we propose UMBRAE, a unified multimodal decoding of brain signals. First, to
extract instance-level conceptual and spatial details from neural signals, we
introduce an efficient universal brain encoder for multimodal-brain alignment
and recover object descriptions at multiple levels of granularity from
subsequent multimodal large language model (MLLM). Second, we introduce a
cross-subject training strategy mapping subject-specific features to a common
feature space. This allows a model to be trained on multiple subjects without
extra resources, even yielding superior results compared to subject-specific
models. Further, we demonstrate this supports weakly-supervised adaptation to
new subjects, with only a fraction of the total training data. Experiments
demonstrate that UMBRAE not only achieves superior results in the newly
introduced tasks but also outperforms methods in well established tasks. To
assess our method, we construct and share with the community a comprehensive
brain understanding benchmark BrainHub. Our code and benchmark are available at
https://weihaox.github.io/UMBRAE.
\\ ( https://arxiv.org/abs/2404.07202 ,  11163kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07204
Date: Wed, 10 Apr 2024 17:59:45 GMT   (9368kb,D)

Title: BRAVE: Broadening the visual encoding of vision-language models
Authors: O\u{g}uzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin
  Kulshrestha, Amir Zamir, Federico Tombari
Categories: cs.CV cs.AI cs.LG
Comments: Project page at https://brave-vlms.epfl.ch/
\\
  Vision-language models (VLMs) are typically composed of a vision encoder,
e.g. CLIP, and a language model (LM) that interprets the encoded features to
solve downstream tasks. Despite remarkable progress, VLMs are subject to
several shortcomings due to the limited capabilities of vision encoders, e.g.
"blindness" to certain image features, visual hallucination, etc. To address
these issues, we study broadening the visual encoding capabilities of VLMs. We
first comprehensively benchmark several vision encoders with different
inductive biases for solving VLM tasks. We observe that there is no single
encoding configuration that consistently achieves top performance across
different tasks, and encoders with different biases can perform surprisingly
similarly. Motivated by this, we introduce a method, named BRAVE, that
consolidates features from multiple frozen encoders into a more versatile
representation that can be directly fed as the input to a frozen LM. BRAVE
achieves state-of-the-art performance on a broad range of captioning and VQA
benchmarks and significantly reduces the aforementioned issues of VLMs, while
requiring a smaller number of trainable parameters than existing methods and
having a more compressed representation. Our results highlight the potential of
incorporating different visual biases for a more broad and contextualized
visual understanding of VLMs.
\\ ( https://arxiv.org/abs/2404.07204 ,  9368kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07206
Date: Wed, 10 Apr 2024 17:59:59 GMT   (31868kb,D)

Title: GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models
Authors: Zewei Zhang, Huan Liu, Jun Chen, Xiangyu Xu
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
\\
  In this paper, we introduce GoodDrag, a novel approach to improve the
stability and image quality of drag editing. Unlike existing methods that
struggle with accumulated perturbations and often result in distortions,
GoodDrag introduces an AlDD framework that alternates between drag and
denoising operations within the diffusion process, effectively improving the
fidelity of the result. We also propose an information-preserving motion
supervision operation that maintains the original features of the starting
point for precise manipulation and artifact reduction. In addition, we
contribute to the benchmarking of drag editing by introducing a new dataset,
Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy
Index and Gemini Score, utilizing Large Multimodal Models. Extensive
experiments demonstrate that the proposed GoodDrag compares favorably against
the state-of-the-art approaches both qualitatively and quantitatively. The
project page is https://gooddrag.github.io.
\\ ( https://arxiv.org/abs/2404.07206 ,  31868kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.06517 (*cross-listing*)
Date: Thu, 4 Apr 2024 05:24:22 GMT   (35505kb,D)

Title: DiffObs: Generative Diffusion for Global Forecasting of Satellite
  Observations
Authors: Jason Stock, Jaideep Pathak, Yair Cohen, Mike Pritchard, Piyush Garg,
  Dale Durran, Morteza Mardani, Noah Brenowitz
Categories: physics.comp-ph cs.CV cs.LG physics.ao-ph stat.ML
Comments: Published as a workshop paper at "Tackling Climate Change with
  Machine Learning", ICLR 2024
\\
  This work presents an autoregressive generative diffusion model (DiffObs) to
predict the global evolution of daily precipitation, trained on a satellite
observational product, and assessed with domain-specific diagnostics. The model
is trained to probabilistically forecast day-ahead precipitation. Nonetheless,
it is stable for multi-month rollouts, which reveal a qualitatively realistic
superposition of convectively coupled wave modes in the tropics. Cross-spectral
analysis confirms successful generation of low frequency variations associated
with the Madden--Julian oscillation, which regulates most subseasonal to
seasonal predictability in the observed atmosphere, and convectively coupled
moist Kelvin waves with approximately correct dispersion relationships. Despite
secondary issues and biases, the results affirm the potential for a next
generation of global diffusion models trained on increasingly sparse, and
increasingly direct and differentiated observations of the world, for practical
applications in subseasonal and climate prediction.
\\ ( https://arxiv.org/abs/2404.06517 ,  35505kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06589 (*cross-listing*)
Date: Tue, 9 Apr 2024 19:33:05 GMT   (540kb,D)

Title: Leveraging Latents for Efficient Thermography Classification and
  Segmentation
Authors: Tamir Shor, Chaim Baskin, Alex Bronstein
Categories: eess.IV cs.CV cs.LG
\\
  Breast cancer is a prominent health concern worldwide, currently being the
secondmost common and second-deadliest type of cancer in women. While current
breast cancer diagnosis mainly relies on mammography imaging, in recent years
the use of thermography for breast cancer imaging has been garnering growing
popularity. Thermographic imaging relies on infrared cameras to capture
body-emitted heat distributions. While these heat signatures have proven useful
for computer-vision systems for accurate breast cancer segmentation and
classification, prior work often relies on handcrafted feature engineering or
complex architectures, potentially limiting the comparability and applicability
of these methods. In this work, we present a novel algorithm for both breast
cancer classification and segmentation. Rather than focusing efforts on manual
feature and architecture engineering, our algorithm focuses on leveraging an
informative, learned feature space, thus making our solution simpler to use and
extend to other frameworks and downstream tasks, as well as more applicable to
data-scarce settings. Our classification produces SOTA results, while we are
the first work to produce segmentation regions studied in this paper.
\\ ( https://arxiv.org/abs/2404.06589 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06638 (*cross-listing*)
Date: Tue, 9 Apr 2024 22:17:20 GMT   (25641kb,D)

Title: SAM-I-Am: Semantic Boosting for Zero-shot Atomic-Scale Electron
  Micrograph Segmentation
Authors: Waqwoya Abebe, Jan Strube, Luanzheng Guo, Nathan R. Tallent, Oceane
  Bel, Steven Spurgeon, Christina Doty, Ali Jannesari
Categories: cond-mat.mtrl-sci cs.CV
\\
  Image segmentation is a critical enabler for tasks ranging from medical
diagnostics to autonomous driving. However, the correct segmentation semantics
- where are boundaries located? what segments are logically similar? - change
depending on the domain, such that state-of-the-art foundation models can
generate meaningless and incorrect results. Moreover, in certain domains,
fine-tuning and retraining techniques are infeasible: obtaining labels is
costly and time-consuming; domain images (micrographs) can be exponentially
diverse; and data sharing (for third-party retraining) is restricted. To enable
rapid adaptation of the best segmentation technology, we propose the concept of
semantic boosting: given a zero-shot foundation model, guide its segmentation
and adjust results to match domain expectations. We apply semantic boosting to
the Segment Anything Model (SAM) to obtain microstructure segmentation for
transmission electron microscopy. Our booster, SAM-I-Am, extracts geometric and
textural features of various intermediate masks to perform mask removal and
mask merging operations. We demonstrate a zero-shot performance increase of
(absolute) +21.35%, +12.6%, +5.27% in mean IoU, and a -9.91%, -18.42%, -4.06%
drop in mean false positive masks across images of three difficulty classes
over vanilla SAM (ViT-L).
\\ ( https://arxiv.org/abs/2404.06638 ,  25641kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06657 (*cross-listing*)
Date: Tue, 9 Apr 2024 23:47:53 GMT   (2555kb,D)

Title: Res-U2Net: Untrained Deep Learning for Phase Retrieval and Image
  Reconstruction
Authors: Carlos Osorio Quero, Daniel Leykam, and Irving Rondon Ojeda
Categories: eess.IV cs.CV physics.optics
Comments: 16 pages, 8 figures, 4 Tables
Journal-ref: Journal of the Optical Society of America A, Vol. 41, Issue 5, pp.
  766-773 (2024)
DOI: 10.1364/JOSAA.511074
\\
  Conventional deep learning-based image reconstruction methods require a large
amount of training data which can be hard to obtain in practice. Untrained deep
learning methods overcome this limitation by training a network to invert a
physical model of the image formation process. Here we present a novel
untrained Res-U2Net model for phase retrieval. We use the extracted phase
information to determine changes in an object's surface and generate a mesh
representation of its 3D structure. We compare the performance of Res-U2Net
phase retrieval against UNet and U2Net using images from the GDXRAY dataset.
\\ ( https://arxiv.org/abs/2404.06657 ,  2555kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06776 (*cross-listing*)
Date: Wed, 10 Apr 2024 06:35:25 GMT   (402kb,D)

Title: Logit Calibration and Feature Contrast for Robust Federated Learning on
  Non-IID Data
Authors: Yu Qiao, Chaoning Zhang, Apurba Adhikary, Choong Seon Hong
Categories: cs.LG cs.AI cs.CV
\\
  Federated learning (FL) is a privacy-preserving distributed framework for
collaborative model training on devices in edge networks. However, challenges
arise due to vulnerability to adversarial examples (AEs) and the
non-independent and identically distributed (non-IID) nature of data
distribution among devices, hindering the deployment of adversarially robust
and accurate learning models at the edge. While adversarial training (AT) is
commonly acknowledged as an effective defense strategy against adversarial
attacks in centralized training, we shed light on the adverse effects of
directly applying AT in FL that can severely compromise accuracy, especially in
non-IID challenges. Given this limitation, this paper proposes FatCC, which
incorporates local logit \underline{C}alibration and global feature
\underline{C}ontrast into the vanilla federated adversarial training
(\underline{FAT}) process from both logit and feature perspectives. This
approach can effectively enhance the federated system's robust accuracy (RA)
and clean accuracy (CA). First, we propose logit calibration, where the logits
are calibrated during local adversarial updates, thereby improving adversarial
robustness. Second, FatCC introduces feature contrast, which involves a global
alignment term that aligns each local representation with unbiased global
features, thus further enhancing robustness and accuracy in federated
adversarial environments. Extensive experiments across multiple datasets
demonstrate that FatCC achieves comparable or superior performance gains in
both CA and RA compared to other baselines.
\\ ( https://arxiv.org/abs/2404.06776 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06941 (*cross-listing*)
Date: Wed, 10 Apr 2024 11:47:51 GMT   (17358kb,D)

Title: Accelerating Cardiac MRI Reconstruction with CMRatt: An Attention-Driven
  Approach
Authors: Anam Hashmi, Julia Dietlmeier, Kathleen M. Curran and Noel E. O'Connor
Categories: eess.IV cs.CV
Comments: This paper has been submitted for the 32nd European Signal Processing
  Conference EUSIPCO 2024 in Lyon
\\
  Cine cardiac magnetic resonance (CMR) imaging is recognised as the benchmark
modality for the comprehensive assessment of cardiac function. Nevertheless,
the acquisition process of cine CMR is considered as an impediment due to its
prolonged scanning time. One commonly used strategy to expedite the acquisition
process is through k-space undersampling, though it comes with a drawback of
introducing aliasing effects in the reconstructed image. Lately, deep
learning-based methods have shown remarkable results over traditional
approaches in rapidly achieving precise CMR reconstructed images. This study
aims to explore the untapped potential of attention mechanisms incorporated
with a deep learning model within the context of the CMR reconstruction
problem. We are motivated by the fact that attention has proven beneficial in
downstream tasks such as image classification and segmentation, but has not
been systematically analysed in the context of CMR reconstruction. Our primary
goal is to identify the strengths and potential limitations of attention
algorithms when integrated with a convolutional backbone model such as a U-Net.
To achieve this, we benchmark different state-of-the-art spatial and channel
attention mechanisms on the CMRxRecon dataset and quantitatively evaluate the
quality of reconstruction using objective metrics. Furthermore, inspired by the
best performing attention mechanism, we propose a new, simple yet effective,
attention pipeline specifically optimised for the task of cardiac image
reconstruction that outperforms other state-of-the-art attention methods. The
layer and model code will be made publicly available.
\\ ( https://arxiv.org/abs/2404.06941 ,  17358kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06991 (*cross-listing*)
Date: Wed, 10 Apr 2024 13:10:52 GMT   (1018kb,D)

Title: Ray-driven Spectral CT Reconstruction Based on Neural Base-Material
  Fields
Authors: Ligen Shi, Chang Liu, Ping Yang, Jun Qiu and Xing Zhao
Categories: eess.IV cs.CV
Comments: 14 pages,16 figures
MSC-class: 68U05, 65D18
ACM-class: I.4.5; I.4.10
\\
  In spectral CT reconstruction, the basis materials decomposition involves
solving a large-scale nonlinear system of integral equations, which is highly
ill-posed mathematically. This paper proposes a model that parameterizes the
attenuation coefficients of the object using a neural field representation,
thereby avoiding the complex calculations of pixel-driven projection
coefficient matrices during the discretization process of line integrals. It
introduces a lightweight discretization method for line integrals based on a
ray-driven neural field, enhancing the accuracy of the integral approximation
during the discretization process. The basis materials are represented as
continuous vector-valued implicit functions to establish a neural field
parameterization model for the basis materials. The auto-differentiation
framework of deep learning is then used to solve the implicit continuous
function of the neural base-material fields. This method is not limited by the
spatial resolution of reconstructed images, and the network has compact and
regular properties. Experimental validation shows that our method performs
exceptionally well in addressing the spectral CT reconstruction. Additionally,
it fulfils the requirements for the generation of high-resolution
reconstruction images.
\\ ( https://arxiv.org/abs/2404.06991 ,  1018kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07110 (*cross-listing*)
Date: Wed, 10 Apr 2024 15:47:35 GMT   (11419kb,D)

Title: Wild Visual Navigation: Fast Traversability Learning via Pre-Trained
  Models and Online Self-Supervision
Authors: Mat\'ias Mattamala and Jonas Frey and Piotr Libera and Nived Chebrolu
  and Georg Martius and Cesar Cadena and Marco Hutter and Maurice Fallon
Categories: cs.RO cs.CV cs.LG
Comments: Extended version of arXiv:2305.08510
\\
  Natural environments such as forests and grasslands are challenging for
robotic navigation because of the false perception of rigid obstacles from high
grass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN),
an online self-supervised learning system for visual traversability estimation.
The system is able to continuously adapt from a short human demonstration in
the field, only using onboard sensing and computing. One of the key ideas to
achieve this is the use of high-dimensional features from pre-trained
self-supervised models, which implicitly encode semantic information that
massively simplifies the learning task. Further, the development of an online
scheme for supervision generator enables concurrent training and inference of
the learned model in the wild. We demonstrate our approach through diverse
real-world deployments in forests, parks, and grasslands. Our system is able to
bootstrap the traversable terrain segmentation in less than 5 min of in-field
training time, enabling the robot to navigate in complex, previously unseen
outdoor terrains. Code: https://bit.ly/498b0CV - Project
page:https://bit.ly/3M6nMHH
\\ ( https://arxiv.org/abs/2404.07110 ,  11419kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07188 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:41:41 GMT   (3613kb,D)

Title: GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on
  FPGA
Authors: Bingyi Zhang, Rajgopal Kannan, Carl Busart, Viktor Prasanna
Categories: cs.DC cs.CV eess.IV
\\
  Graph neural networks (GNNs) have recently empowered various novel computer
vision (CV) tasks. In GNN-based CV tasks, a combination of CNN layers and GNN
layers or only GNN layers are employed. This paper introduces GCV-Turbo, a
domain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV
tasks. GCV-Turbo consists of two key components: (1) a \emph{novel} hardware
architecture optimized for the computation kernels in both CNNs and GNNs using
the same set of computation resources. (2) a PyTorch-compatible compiler that
takes a user-defined model as input, performs end-to-end optimization for the
computation graph of a given GNN-based CV task, and produces optimized code for
hardware execution. The hardware architecture and the compiler work
synergistically to support a variety of GNN-based CV tasks. We implement
GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six
representative GNN-based CV tasks with diverse input data modalities (e.g.,
image, human skeleton, point cloud). Compared with state-of-the-art CPU (GPU)
implementations, GCV-Turbo achieves an average latency reduction of
$68.4\times$ ($4.1\times$) on these six GNN-based CV tasks. Moreover, GCV-Turbo
supports the execution of the standalone CNNs or GNNs, achieving performance
comparable to that of state-of-the-art CNN (GNN) accelerators for widely used
CNN-only (GNN-only) models.
\\ ( https://arxiv.org/abs/2404.07188 ,  3613kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2208.11650
replaced with revised version Tue, 9 Apr 2024 17:59:34 GMT   (13732kb,D)

Title: Lane Change Classification and Prediction with Action Recognition
  Networks
Authors: Kai Liang, Jun Wang and Abhir Bhalerao
Categories: cs.CV
Comments: Accepted to ECCV2022 AVVISION
\\ ( https://arxiv.org/abs/2208.11650 ,  13732kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16101
replaced with revised version Wed, 10 Apr 2024 02:33:57 GMT   (10852kb,D)

Title: A Generic Shared Attention Mechanism for Various Backbone Neural
  Networks
Authors: Zhongzhan Huang, Senwei Liang, Mingfu Liang, Liang Lin
Categories: cs.CV cs.AI
Comments: Work in progress. arXiv admin note: text overlap with
  arXiv:1905.10671
\\ ( https://arxiv.org/abs/2210.16101 ,  10852kb)
------------------------------------------------------------------------------
\\
arXiv:2212.05140
replaced with revised version Tue, 9 Apr 2024 19:17:07 GMT   (614kb,D)

Title: Local Neighborhood Features for 3D Classification
Authors: Shivanand Venkanna Sheshappanavar and Chandra Kambhamettu
Categories: cs.CV cs.MM
\\ ( https://arxiv.org/abs/2212.05140 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2212.11120
replaced with revised version Wed, 10 Apr 2024 17:15:23 GMT   (7776kb,D)

Title: Deep Learning for Inertial Sensor Alignment
Authors: Maxim Freydin, Niv Sfaradi, Nimrod Segol, Areej Eweida, and Barak Or
Categories: cs.CV cs.LG cs.RO
Comments: 9 Pages, Preprint. Accepted IEEE
DOI: 10.1109/JSEN.2024.3384302
\\ ( https://arxiv.org/abs/2212.11120 ,  7776kb)
------------------------------------------------------------------------------
\\
arXiv:2301.04218
replaced with revised version Wed, 10 Apr 2024 01:11:15 GMT   (13309kb,D)

Title: Leveraging Diffusion For Strong and High Quality Face Morphing Attacks
Authors: Zander W. Blasingame and Chen Liu
Categories: cs.CV cs.CR cs.LG
Comments: Diffusion Morphs (DiM) paper. Accepted in IEEE TBIOM
Journal-ref: IEEE Transactions on Biometrics, Behavior, and Identity Science (
  Volume: 6, Issue: 1, January 2024)
DOI: 10.1109/TBIOM.2024.3349857
\\ ( https://arxiv.org/abs/2301.04218 ,  13309kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10035
replaced with revised version Wed, 10 Apr 2024 09:34:03 GMT   (6659kb,D)

Title: Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey
Authors: Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei,
  Yaowei Wang, Yonghong Tian, Wen Gao
Categories: cs.CV cs.AI cs.MM
Comments: Accepted by Machine Intelligence Research (MIR)
\\ ( https://arxiv.org/abs/2302.10035 ,  6659kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00977
replaced with revised version Wed, 10 Apr 2024 10:56:00 GMT   (47018kb,D)

Title: AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation
Authors: Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann,
  Bastian Leibe, Konrad Schindler, Theodora Kontogianni
Categories: cs.CV cs.HC
Comments: ICLR 2024 camera-ready. Project page: https://ywyue.github.io/AGILE3D
\\ ( https://arxiv.org/abs/2306.00977 ,  47018kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10798
replaced with revised version Wed, 10 Apr 2024 11:42:22 GMT   (15765kb,D)

Title: ExpPoint-MAE: Better interpretability and performance for
  self-supervised point cloud transformers
Authors: Ioannis Romanelis and Vlassis Fotis and Konstantinos Moustakas and
  Adrian Munteanu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.10798 ,  15765kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06038
replaced with revised version Wed, 10 Apr 2024 03:27:04 GMT   (1210kb,D)

Title: Pyramid Deep Fusion Network for Two-Hand Reconstruction from RGB-D
  Images
Authors: Jinwei Ren, and Jianke Zhu
Categories: cs.CV
Comments: Accepted by TCSVT
Journal-ref: IEEE Transactions on Circuits and Systems for Video Technology,
  2024
DOI: 10.1109/TCSVT.2024.3369646
\\ ( https://arxiv.org/abs/2307.06038 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12220
replaced with revised version Wed, 10 Apr 2024 13:15:41 GMT   (1824kb)

Title: Expediting Building Footprint Extraction from High-resolution Remote
  Sensing Images via progressive lenient supervision
Authors: Haonan Guo, Bo Du, Chen Wu, Xin Su, Liangpei Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.12220 ,  1824kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12256
replaced with revised version Wed, 10 Apr 2024 13:43:54 GMT   (2617kb)

Title: Building-road Collaborative Extraction from Remotely Sensed Images via
  Cross-Interaction
Authors: Haonan Guo, Xin Su, Chen Wu, Bo Du, Liangpei Zhang
Categories: cs.CV cs.AI
Comments: IEEE Transactions on Geoscience and Remote Sensing
Journal-ref: IEEE Transactions on Geoscience and Remote Sensing
DOI: 10.1109/TGRS.2024.3383057
\\ ( https://arxiv.org/abs/2307.12256 ,  2617kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10610
replaced with revised version Wed, 10 Apr 2024 08:16:18 GMT   (21631kb,D)

Title: Ear-Keeper: Real-time Diagnosis of Ear Lesions Utilizing
  Ultralight-Ultrafast ConvNet and Large-scale Ear Endoscopic Dataset
Authors: Yubiao Yue, Xinyu Zeng, Xiaoqiang Shi, Meiping Zhang, Fan Zhang,
  Yunxin Liang, Yan Liu, Zhenzhang Li, Yang Li
Categories: cs.CV cs.SE
Comments: 18 pages,8 figures
\\ ( https://arxiv.org/abs/2308.10610 ,  21631kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05645
replaced with revised version Wed, 10 Apr 2024 03:05:04 GMT   (16560kb,D)

Title: CitDet: A Benchmark Dataset for Citrus Fruit Detection
Authors: Jordan A. James, Heather K. Manching, Matthew R. Mattia, Kim D.
  Bowman, Amanda M. Hulse-Kemp, William J. Beksi
Categories: cs.CV cs.RO
Comments: Submitted to IEEE Robotics and Automation Letters (RA-L)
\\ ( https://arxiv.org/abs/2309.05645 ,  16560kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02265
replaced with revised version Wed, 10 Apr 2024 12:54:12 GMT   (26280kb,D)

Title: DREAM: Visual Decoding from Reversing Human Visual System
Authors: Weihao Xia, Raoul de Charette, Cengiz \"Oztireli, Jing-Hao Xue
Categories: cs.CV cs.LG eess.IV q-bio.NC
Comments: Project Page: https://weihaox.github.io/DREAM
\\ ( https://arxiv.org/abs/2310.02265 ,  26280kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12796
replaced with revised version Wed, 10 Apr 2024 10:37:22 GMT   (9264kb,D)

Title: Physics-guided Shape-from-Template: Monocular Video Perception through
  Neural Surrogate Models
Authors: David Stotko, Nils Wandel, Reinhard Klein
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.12796 ,  9264kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00825
replaced with revised version Tue, 9 Apr 2024 23:28:49 GMT   (41148kb,D)

Title: SocialCounterfactuals: Probing and Mitigating Intersectional Social
  Biases in Vision-Language Models with Counterfactual Examples
Authors: Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Anahita
  Bhiwandiwalla, and Vasudev Lal
Categories: cs.CV cs.AI
Comments: Accepted to CVPR 2024. arXiv admin note: text overlap with
  arXiv:2310.02988
\\ ( https://arxiv.org/abs/2312.00825 ,  41148kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03502
replaced with revised version Wed, 10 Apr 2024 08:29:23 GMT   (18305kb,D)

Title: Improving the Generalization of Segmentation Foundation Model under
  Distribution Shift via Weakly Supervised Adaptation
Authors: Haojie Zhang, Yongyi Su, Xun Xu and Kui Jia
Categories: cs.CV
Comments: 20 pages, 12 figures
\\ ( https://arxiv.org/abs/2312.03502 ,  18305kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04746
replaced with revised version Tue, 9 Apr 2024 21:48:42 GMT   (21487kb,D)

Title: Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized
  Narratives from Open-Source Histopathology Videos
Authors: Mehmet Saygin Seyfioglu, Wisdom O. Ikezogwo, Fatemeh Ghezloo, Ranjay
  Krishna, Linda Shapiro
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.04746 ,  21487kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06275
replaced with revised version Wed, 10 Apr 2024 11:49:05 GMT   (551kb,D)

Title: DG-TTA: Out-of-domain medical image segmentation through Domain
  Generalization and Test-Time Adaptation
Authors: Christian Weihsbach, Christian N. Kruse, Alexander Bigalke, Mattias P.
  Heinrich
Categories: cs.CV cs.LG
MSC-class: 92C55 (Primary), 68T07 (Secondary)
ACM-class: I.2.6; I.4.6
\\ ( https://arxiv.org/abs/2312.06275 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07937
replaced with revised version Wed, 10 Apr 2024 13:35:51 GMT   (19419kb,D)

Title: BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics
Authors: Wenqian Zhang, Molin Huang, Yuxuan Zhou, Juze Zhang, Jingyi Yu, Jingya
  Wang, Lan Xu
Categories: cs.CV
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2312.07937 ,  19419kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08514
replaced with revised version Tue, 9 Apr 2024 18:23:39 GMT   (13861kb,D)

Title: TAM-VT: Transformation-Aware Multi-scale Video Transformer for
  Segmentation and Tracking
Authors: Raghav Goyal, Wan-Cyuan Fan, Mennatullah Siam, Leonid Sigal
Categories: cs.CV
\\ ( https://arxiv.org/abs/2312.08514 ,  13861kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10908
replaced with revised version Wed, 10 Apr 2024 15:59:31 GMT   (5963kb,D)

Title: CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update
Authors: Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun
  Zhu, Qing Li
Categories: cs.CV
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2312.10908 ,  5963kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04350
replaced with revised version Wed, 10 Apr 2024 11:58:24 GMT   (2457kb,D)

Title: Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial
  Robustness
Authors: Sibo Wang, Jie Zhang, Zheng Yuan, Shiguang Shan
Categories: cs.CV
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2401.04350 ,  2457kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07745
replaced with revised version Wed, 10 Apr 2024 15:30:23 GMT   (6995kb,D)

Title: MaskClustering: View Consensus based Mask Graph Clustering for
  Open-Vocabulary 3D Instance Segmentation
Authors: Mi Yan, Jiazhao Zhang, Yan Zhu, He Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2401.07745 ,  6995kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10166
replaced with revised version Wed, 10 Apr 2024 14:25:12 GMT   (5993kb,D)

Title: VMamba: Visual State Space Model
Authors: Yue Liu and Yunjie Tian and Yuzhong Zhao and Hongtian Yu and Lingxi
  Xie and Yaowei Wang and Qixiang Ye and Yunfan Liu
Categories: cs.CV
Comments: 21 pages, 12 figures, 5 tables
\\ ( https://arxiv.org/abs/2401.10166 ,  5993kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10831
replaced with revised version Wed, 10 Apr 2024 15:19:07 GMT   (23960kb,D)

Title: Understanding Video Transformers via Universal Concept Discovery
Authors: Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos
  G. Derpanis, Pavel Tokmakov
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: CVPR 2024 (Highlight)
\\ ( https://arxiv.org/abs/2401.10831 ,  23960kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05195
replaced with revised version Tue, 9 Apr 2024 22:14:37 GMT   (36356kb,D)

Title: $\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion
  Models by Leveraging CLIP Latent Space
Authors: Maitreya Patel, Sangmin Jung, Chitta Baral, Yezhou Yang
Categories: cs.CV cs.CL
Comments: Project page: https://eclipse-t2i.github.io/Lambda-ECLIPSE/
\\ ( https://arxiv.org/abs/2402.05195 ,  36356kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18320
replaced with revised version Wed, 10 Apr 2024 15:09:22 GMT   (14613kb,D)

Title: Location-guided Head Pose Estimation for Fisheye Image
Authors: Bing Li, Dong Zhang, Cheng Huang, Yun Xian, Ming Li, and Dah-Jye Lee
Categories: cs.CV cs.AI
Comments: Revised Introduction and Related Work; Submitted to lEEE Transactions
  on Cognitive and Developmental Systems for review
\\ ( https://arxiv.org/abs/2402.18320 ,  14613kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02527
replaced with revised version Tue, 9 Apr 2024 18:26:27 GMT   (1000kb)

Title: A dataset of over one thousand computed tomography scans of battery
  cells
Authors: Amariah Condon, Bailey Buscarino, Eric Moch, William J. Sehnert, Owen
  Miles, Patrick K. Herring, Peter M. Attia
Categories: cs.CV cs.CE
\\ ( https://arxiv.org/abs/2403.02527 ,  1000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03190
replaced with revised version Wed, 10 Apr 2024 09:51:11 GMT   (2350kb,D)

Title: Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract
  Reasoning process
Authors: Ruizhuo Song, Beiming Yuan
Categories: cs.CV
Comments: 14 pages, 14 figures, 5 tables
\\ ( https://arxiv.org/abs/2403.03190 ,  2350kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05916
replaced with revised version Wed, 10 Apr 2024 07:58:44 GMT   (4941kb,D)

Title: GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual
  Affective Computing
Authors: Hao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang,
  Yuting Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, Dengbo He, Shuiguang Deng,
  Hao Chen, Yingcong Chen, Shiguang Shan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.05916 ,  4941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08092
replaced with revised version Tue, 9 Apr 2024 20:55:01 GMT   (44291kb,D)

Title: Mitigating the Impact of Attribute Editing on Face Recognition
Authors: Sudipta Banerjee, Sai Pranaswi Mullangi, Shruti Wagle, Chinmay Hegde,
  Nasir Memon
Categories: cs.CV
Comments: Under review
\\ ( https://arxiv.org/abs/2403.08092 ,  44291kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11848
replaced with revised version Wed, 10 Apr 2024 04:05:24 GMT   (30537kb,D)

Title: GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object
  Detection
Authors: Ziying Song, Lei Yang, Shaoqing Xu, Lin Liu, Dongyang Xu, Caiyan Jia,
  Feiyang Jia, Li Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2403.11848 ,  30537kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02065
replaced with revised version Wed, 10 Apr 2024 01:53:17 GMT   (28047kb,D)

Title: Multi-Level Label Correction by Distilling Proximate Patterns for
  Semi-supervised Semantic Segmentation
Authors: Hui Xiao, Yuting Hong, Li Dong, Diqun Yan, Jiayan Zhuang, Junjie
  Xiong, Dongtai Liang and Chengbin Peng
Categories: cs.CV
Comments: 12 pages, 8 figures. IEEE Transactions on Multimedia, 2024
DOI: 10.1109/TMM.2024.3374594
\\ ( https://arxiv.org/abs/2404.02065 ,  28047kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02233
replaced with revised version Wed, 10 Apr 2024 15:22:05 GMT   (42470kb,D)

Title: Visual Concept Connectome (VCC): Open World Concept Discovery and their
  Interlayer Connections in Deep Models
Authors: Matthew Kowal, Richard P. Wildes, Konstantinos G. Derpanis
Categories: cs.CV
Comments: CVPR 2024 (Highlight)
\\ ( https://arxiv.org/abs/2404.02233 ,  42470kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02668
replaced with revised version Wed, 10 Apr 2024 08:47:32 GMT   (3639kb,D)

Title: RS-Mamba for Large Remote Sensing Image Dense Prediction
Authors: Sijie Zhao, Hao Chen, Xueliang Zhang, Pengfeng Xiao, Lei Bai, and
  Wanli Ouyang
Categories: cs.CV
Comments: 15 pages,8 figures
\\ ( https://arxiv.org/abs/2404.02668 ,  3639kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03384
replaced with revised version Wed, 10 Apr 2024 04:24:36 GMT   (6491kb,D)

Title: LongVLM: Efficient Long Video Understanding via Large Language Models
Authors: Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, Bohan Zhuang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2404.03384 ,  6491kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04125
replaced with revised version Mon, 8 Apr 2024 21:14:43 GMT   (37863kb,D)

Title: No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency
  Determines Multimodal Model Performance
Authors: Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip
  H.S. Torr, Adel Bibi, Samuel Albanie, Matthias Bethge
Categories: cs.CV cs.CL cs.LG
Comments: Extended version of the short paper accepted at DPFM, ICLR'24
\\ ( https://arxiv.org/abs/2404.04125 ,  37863kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05063
replaced with revised version Wed, 10 Apr 2024 07:44:40 GMT   (6324kb,D)

Title: AUEditNet: Dual-Branch Facial Action Unit Intensity Manipulation with
  Implicit Disentanglement
Authors: Shiwei Jin, Zhen Wang, Lei Wang, Peng Liu, Ning Bi, Truong Nguyen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2404.05063 ,  6324kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05139
replaced with revised version Tue, 9 Apr 2024 23:17:07 GMT   (8831kb,D)

Title: Better Monocular 3D Detectors with LiDAR from the Past
Authors: Yurong You, Cheng Perng Phoo, Carlos Andres Diaz-Ruiz, Katie Z Luo,
  Wei-Lun Chao, Mark Campbell, Bharath Hariharan, Kilian Q Weinberger
Categories: cs.CV cs.RO
Comments: Accepted by ICRA 2024. The code can be found at
  https://github.com/YurongYou/AsyncDepth
\\ ( https://arxiv.org/abs/2404.05139 ,  8831kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05215
replaced with revised version Wed, 10 Apr 2024 00:49:11 GMT   (19226kb,D)

Title: Spatio-Temporal Attention and Gaussian Processes for Personalized Video
  Gaze Estimation
Authors: Swati Jindal, Mohit Yadav, Roberto Manduchi
Categories: cs.CV
Comments: Accepted at CVPR 2024 Gaze workshop
\\ ( https://arxiv.org/abs/2404.05215 ,  19226kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05317
replaced with revised version Wed, 10 Apr 2024 13:30:09 GMT   (259kb,D)

Title: WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A
  Conceptual Architecture
Authors: Giuseppe Macario
Categories: cs.CV cs.GR cs.HC cs.MM
Comments: minor fixes (typos, URLs etc.)
\\ ( https://arxiv.org/abs/2404.05317 ,  259kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05490
replaced with revised version Tue, 9 Apr 2024 18:55:43 GMT   (2032kb,D)

Title: Two-Person Interaction Augmentation with Skeleton Priors
Authors: Baiyi Li, Edmond S.L. Ho, Hubert P.H. Shum and He Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2404.05490 ,  2032kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06033
replaced with revised version Wed, 10 Apr 2024 12:55:49 GMT   (17385kb,D)

Title: Little Strokes Fell Great Oaks: Boosting the Hierarchical Features for
  Multi-exposure Image Fusion
Authors: Pan Mu, Zhiying Du, Jinyuan Liu, Cong Bai
Categories: cs.CV
Journal-ref: Proceedings of the 31st ACM International Conference on
  Multimedia, October 2023, Pages 2985-2993
DOI: 10.1145/3581783.3612561
\\ ( https://arxiv.org/abs/2404.06033 ,  17385kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06194
replaced with revised version Wed, 10 Apr 2024 04:01:43 GMT   (1438kb,D)

Title: Exploring the Potential of Large Foundation Models for Open-Vocabulary
  HOI Detection
Authors: Ting Lei, Shaofeng Yin, Yang Liu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2404.06194 ,  1438kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06493
replaced with revised version Wed, 10 Apr 2024 02:24:58 GMT   (19167kb,D)

Title: Flying with Photons: Rendering Novel Views of Propagating Light
Authors: Anagh Malik, Noah Juravsky, Ryan Po, Gordon Wetzstein, Kiriakos N.
  Kutulakos, David B. Lindell
Categories: cs.CV eess.IV
Comments: Project page: https://anaghmalik.com/FlyingWithPhotons/
\\ ( https://arxiv.org/abs/2404.06493 ,  19167kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06507
replaced with revised version Wed, 10 Apr 2024 02:23:09 GMT   (7287kb,D)

Title: Reconstructing Hand-Held Objects in 3D
Authors: Jane Wu, Georgios Pavlakos, Georgia Gkioxari, Jitendra Malik
Categories: cs.CV
Comments: Project page: https://janehwu.github.io/mcc-ho
\\ ( https://arxiv.org/abs/2404.06507 ,  7287kb)
------------------------------------------------------------------------------
\\
arXiv:2211.02736
replaced with revised version Wed, 10 Apr 2024 04:51:33 GMT   (2196kb,D)

Title: Discovering Closed-Loop Failures of Vision-Based Controllers via
  Reachability Analysis
Authors: Kaustav Chakraborty and Somil Bansal
Categories: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY
Journal-ref: IEEE Robotics and Automation Letters 8.5 (2023): 2692-2699
DOI: 10.1109/LRA.2023.3258719
\\ ( https://arxiv.org/abs/2211.02736 ,  2196kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14855
replaced with revised version Wed, 10 Apr 2024 17:35:16 GMT   (21779kb,D)

Title: Disentangled Explanations of Neural Network Predictions by Finding
  Relevant Subspaces
Authors: Pattarawat Chormai, Jan Herrmann, Klaus-Robert M\"uller, Gr\'egoire
  Montavon
Categories: cs.LG cs.AI cs.CV
Comments: 17 pages + supplement
\\ ( https://arxiv.org/abs/2212.14855 ,  21779kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01289 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 04:42:10 GMT   (28816kb,D)

Title: nnMobileNe: Rethinking CNN for Retinopathy Research
Authors: Wenhui Zhu, Peijie Qiu, Xiwen Chen, Xin Li, Natasha Lepore, Oana M.
  Dumitrascu, Yalin Wang
Categories: eess.IV cs.CV
Comments: Accepted as a conference paper to 2024 CVPRW
\\ ( https://arxiv.org/abs/2306.01289 ,  28816kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07887 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 10:06:46 GMT   (18434kb,D)

Title: Unsupervised Denoising for Signal-Dependent and Row-Correlated Imaging
  Noise
Authors: Benjamin Salmon and Alexander Krull
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2310.07887 ,  18434kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10568 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 02:19:19 GMT   (42674kb,D)

Title: Phase Guided Light Field for Spatial-Depth High Resolution 3D Imaging
Authors: Geyou Zhang, Ce Zhu, Kai Liu, Yipeng Liu
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2311.10568 ,  42674kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15361
replaced with revised version Wed, 10 Apr 2024 06:46:08 GMT   (32208kb,D)

Title: Ultra-Range Gesture Recognition using a Web-Camera in Human-Robot
  Interaction
Authors: Eran Bamani, Eden Nissinman, Inbar Meir, Lisa Koenigsberg, Avishai
  Sintov
Categories: cs.RO cs.CV
Comments: Engineering Applications of Artificial Intelligence, In press
\\ ( https://arxiv.org/abs/2311.15361 ,  32208kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00068
replaced with revised version Wed, 10 Apr 2024 16:04:48 GMT   (16474kb,D)

Title: GLiDR: Topologically Regularized Graph Generative Network for Sparse
  LiDAR Point Clouds
Authors: Prashant Kumar, Kshitij Madhav Bhat, Vedang Bhupesh Shenvi Nadkarni,
  Prem Kalra
Categories: cs.RO cs.CV
Comments: IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)
\\ ( https://arxiv.org/abs/2312.00068 ,  16474kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10144
replaced with revised version Wed, 10 Apr 2024 13:58:08 GMT   (917kb,D)

Title: Data-Efficient Multimodal Fusion on a Single GPU
Authors: No\"el Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin
  Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims
  Volkovs
Categories: cs.LG cs.AI cs.CV
Comments: CVPR 2024 (Highlight)
\\ ( https://arxiv.org/abs/2312.10144 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11468 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 15:58:09 GMT   (16736kb,D)

Title: Bias-Reduced Neural Networks for Parameter Estimation in Quantitative
  MRI
Authors: Andrew Mao, Sebastian Flassbeck, Jakob Assl\"ander
Categories: physics.med-ph cs.CV
\\ ( https://arxiv.org/abs/2312.11468 ,  16736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02263
replaced with revised version Wed, 10 Apr 2024 09:00:44 GMT   (1738kb,D)

Title: MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly
  Mixed Classifiers
Authors: Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi
Categories: cs.LG cs.AI cs.CV
MSC-class: 68T07
\\ ( https://arxiv.org/abs/2402.02263 ,  1738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07354 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 07:54:14 GMT   (2072kb,D)

Title: Re-DiffiNet: Modeling discrepancies in tumor segmentation using
  diffusion models
Authors: Tianyi Ren, Abhishek Sharma, Juampablo Heras Rivera, Harshitha Rebala,
  Ethan Honey, Agamdeep Chopra, Jacob Ruzevick, Mehmet Kurt
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2402.07354 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08551 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 07:58:04 GMT   (1412kb,D)

Title: GaussianImage: 1000 FPS Image Representation and Compression by 2D
  Gaussian Splatting
Authors: Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei
  Qin, Guo Lu, Jing Geng, Jun Zhang
Categories: eess.IV cs.AI cs.CV cs.MM
\\ ( https://arxiv.org/abs/2403.08551 ,  1412kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01102 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 19:26:36 GMT   (1226kb,D)

Title: Diffusion based Zero-shot Medical Image-to-Image Translation for Cross
  Modality Segmentation
Authors: Zihao Wang, Yingyu Yang, Yuzhou Chen, Tingting Yuan, Maxime Sermesant,
  Herve Delingette, Ona Wu
Categories: eess.IV cs.CV cs.LG
Comments: Neurips 2023 Diffusion Workshop
\\ ( https://arxiv.org/abs/2404.01102 ,  1226kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01563 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 13:02:59 GMT   (684kb)

Title: Two-Phase Multi-Dose-Level PET Image Reconstruction with Dose Level
  Awareness
Authors: Yuchen Fei, Yanmei Luo, Yan Wang, Jiaqi Cui, Yuanyuan Xu, Jiliu Zhou,
  Dinggang Shen
Categories: eess.IV cs.CV
Comments: Accepted by ISBI2024
\\ ( https://arxiv.org/abs/2404.01563 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01929 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 03:36:33 GMT   (485kb)

Title: Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA -- A
  Semi-Supervised Video Object Detection Method
Authors: Jyun-An Lin, Yun-Chien Cheng, Ching-Kai Lin
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2404.01929 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06080 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 03:35:35 GMT   (352kb)

Title: Using Few-Shot Learning to Classify Primary Lung Cancer and Other
  Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial
  Ultrasound Procedures
Authors: Ching-Kai Lin, Di-Chun Wei, Yun-Chien Cheng
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2404.06080 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06479
replaced with revised version Wed, 10 Apr 2024 02:12:27 GMT   (2755kb,D)

Title: Text-Based Reasoning About Vector Graphics
Authors: Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li,
  Jiajun Wu, Heng Ji
Categories: cs.CL cs.AI cs.CV
Comments: Project page: https://mikewangwzhl.github.io/VDLM/
\\ ( https://arxiv.org/abs/2404.06479 ,  2755kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
